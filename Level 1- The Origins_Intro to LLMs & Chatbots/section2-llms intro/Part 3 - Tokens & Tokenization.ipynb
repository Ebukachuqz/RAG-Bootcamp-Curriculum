{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3cd568-6f0c-4139-8b3b-4c3580d0762f",
   "metadata": {},
   "source": [
    "# **Level 1: The Origins — Intro to LLMs & Chatbots**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c370f-e600-4f24-8485-ece320bf166f",
   "metadata": {},
   "source": [
    "## **Section 2: Introduction to Language Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43ac1e-e1aa-41b4-90e4-c1a25cb63153",
   "metadata": {},
   "source": [
    "### **Part 3: Tokens & Tokenization**\n",
    "\n",
    "---\n",
    "\n",
    "Before we can understand how AI models like ChatGPT process language, we need to appreciate a simple but crucial fact: **computers don’t understand human language the way we do**.\n",
    "\n",
    "We see language as sentences, ideas, and meaning. Computers, on the other hand, deal with numbers and symbols. To bridge that gap, the first step in building modern AI systems that understand text is **breaking down language into smaller, manageable pieces**. These pieces are called **tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### **What are Tokens?**\n",
    "\n",
    "In simple terms, a **token** is a unit of text that the model processes. Depending on the model and its design, a token can be:\n",
    "\n",
    "* A full word (e.g., \"cat\")\n",
    "* Part of a word (e.g., \"inter\" and \"national\" from \"international\")\n",
    "* Punctuation (e.g., \".\")\n",
    "* Special symbols (e.g., `<|endoftext|>`)\n",
    "\n",
    "Tokens are the building blocks of language for AI models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Not Just Use Whole Words?**\n",
    "\n",
    "Language is complex. Words can be long, short, combined, or made-up. If we treated only whole words as units, the model would struggle with:\n",
    "\n",
    "* Rare words\n",
    "* Misspellings\n",
    "* New words never seen before\n",
    "\n",
    "Instead, breaking text into smaller chunks (tokens) allows the model to handle language flexibly. Even if it has never seen the exact word \"antidisestablishmentarianism,\" it can process its tokens and still understand parts of it.\n",
    "\n",
    "---\n",
    "\n",
    "### **How is Text Broken into Tokens?**\n",
    "\n",
    "This process is called **tokenization**. A special algorithm breaks text into tokens according to predefined rules.\n",
    "\n",
    "Different models use different tokenization strategies:\n",
    "\n",
    "* Some use **WordPiece** (common in BERT models)\n",
    "* Others use **Byte Pair Encoding (BPE)** (common in GPT models)\n",
    "* Some use **SentencePiece** (common in multilingual models)\n",
    "\n",
    "These methods aim to balance efficiency and flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Illustration Example:**\n",
    "\n",
    "Take the sentence:\n",
    "*\"I love international collaborations.\"*\n",
    "\n",
    "A tokenization algorithm might break it down like this:\n",
    "\n",
    "\\[`I`, `love`, `inter`, `national`, `collaborations`, `.`]\n",
    "\n",
    "Notice how:\n",
    "\n",
    "* \"international\" becomes two tokens: \"inter\" and \"national\"\n",
    "* Punctuation is kept as its own token\n",
    "\n",
    "Alternatively, depending on the tokenizer, it might also look like:\n",
    "\\[`I`, `love`, `international`, `collaborations`, `.`]\n",
    "\n",
    "The key takeaway: tokenization isn't always perfectly intuitive to humans, but it's optimized for the model to handle language efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Does Token Count Matter?**\n",
    "\n",
    "Modern AI models process tokens one at a time, internally converting them into numerical representations the model can work with. However, they have a **maximum token limit**, known as the **context window**.\n",
    "\n",
    "This limit defines how much text the model can handle at once. For example:\n",
    "\n",
    "* GPT-3.5 has a limit of around **4,000 tokens**\n",
    "* GPT-4 can handle up to **128,000 tokens** in some versions\n",
    "\n",
    "If your text exceeds this limit, the model will:\n",
    "\n",
    "* Truncate the beginning or end\n",
    "* Lose context\n",
    "* Be unable to process the full input\n",
    "\n",
    "This is why understanding tokens is important, especially when building applications or chatbots that work with long text.\n",
    "\n",
    "---\n",
    "\n",
    "**Real-World Implication:**\n",
    "\n",
    "Imagine you're building a chatbot to summarize legal contracts. If the contract is too long and exceeds the token limit, the chatbot won’t see the entire document — leading to incomplete or inaccurate responses.\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick Clarifications:**\n",
    "\n",
    "* **Tokens ≠ Characters.** A single token might contain multiple characters, or a single character might be its own token.\n",
    "* **Token count ≠ Word count.** A sentence with five words may have 5, 7, or more tokens depending on the tokenizer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "* Tokens are the basic units of text that AI models process.\n",
    "* Tokenization breaks text into these chunks.\n",
    "* The token limit defines how much information a model can process at once.\n",
    "* Understanding tokens helps you design better AI applications and prevents errors due to exceeding context limits.\n",
    "\n",
    "---\n",
    "\n",
    "In the next part, we'll build on this by exploring the **Transformer**, the engine that processes these tokens and enables models to understand language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c9703-0fb4-4697-8308-225cd5b310b6",
   "metadata": {},
   "source": [
    "### **Part 3: Tokens & Tokenization (Technical Subpart)**\n",
    "\n",
    "To fully grasp how AI models handle language, it’s not enough to know that text gets broken into \"tokens\" — we must also understand **how** this happens under the hood. This process, known as **tokenization**, is fundamental to how language models process and generate text.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formal Definition of Tokenization:**\n",
    "\n",
    "**Tokenization** is the process of mapping raw text input into discrete, machine-understandable units called **tokens**, using a deterministic algorithm. These tokens serve as indices or inputs to the model's embedding layer.\n",
    "\n",
    "Mathematically, we can express tokenization as:\n",
    "\n",
    "$$\n",
    "T(x) = [t_1, t_2, t_3, \\ldots, t_n]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $x$ = raw text input (string)\n",
    "* $T(x)$ = list of tokens produced\n",
    "* $t_1, t_2, \\ldots, t_n$ = individual tokens\n",
    "\n",
    "The output tokens are not raw text — they are mapped to integer IDs using a vocabulary, ready for model consumption.\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Tokenization Algorithms:**\n",
    "\n",
    "Different tokenizers use different rules for breaking text into tokens. Here are the most common types:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Word-Level Tokenization**\n",
    "\n",
    "* Each word is treated as a token.\n",
    "* Simple but inefficient for rare words.\n",
    "* Example:\n",
    "  *\"The quick brown fox\"* → $`The`, `quick`, `brown`, `fox`$\n",
    "\n",
    "**Limitation:**\n",
    "\n",
    "* New words, typos, or rare vocabulary lead to unknown tokens (`<UNK>`), reducing model robustness.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Subword Tokenization (Most Common Today)**\n",
    "\n",
    "Subword tokenizers break text into **frequent subword units**, which balances vocabulary size and flexibility. This allows the model to handle rare or unseen words by decomposing them.\n",
    "\n",
    "Popular subword tokenizers:\n",
    "\n",
    "* **Byte-Pair Encoding (BPE)**\n",
    "* **WordPiece**\n",
    "* **Unigram Language Model**\n",
    "* **SentencePiece**\n",
    "\n",
    "---\n",
    "\n",
    "##### **Byte-Pair Encoding (BPE) – Conceptual Explanation:**\n",
    "\n",
    "BPE starts with individual characters as tokens and iteratively merges the most frequent pairs of tokens into new tokens. This continues until a fixed vocabulary size is reached.\n",
    "\n",
    "**Illustrative Example:**\n",
    "Suppose we have the word \"internationalization\" seen repeatedly during training.\n",
    "\n",
    "BPE might produce tokens like:\n",
    "$`inter`, `national`, `ization`$\n",
    "\n",
    "This allows the model to handle variations like:\n",
    "\n",
    "* \"international\" → $`inter`, `national`$\n",
    "* \"nationalism\" → $`national`, `ism`$\n",
    "\n",
    "**Mathematical Perspective:**\n",
    "\n",
    "* Initialize token set: all characters\n",
    "* For $N$ iterations:\n",
    "\n",
    "  * Find the most frequent pair of consecutive tokens\n",
    "  * Merge them into a new token\n",
    "* Result: a compact, efficient token vocabulary\n",
    "\n",
    "---\n",
    "\n",
    "##### **WordPiece and Unigram Models:**\n",
    "\n",
    "These use similar ideas but differ in the specifics of how merges or probabilities are selected. For example:\n",
    "\n",
    "* **WordPiece:** Popular in BERT; builds vocabulary by considering word likelihood improvements.\n",
    "* **Unigram Model:** Used by SentencePiece; selects subword units probabilistically to maximize text likelihood.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Byte-Level Tokenization (e.g., GPT-2, GPT-3, GPT-4 Tokenizers)**\n",
    "\n",
    "For maximum robustness, modern LLMs like GPT-3.5 and GPT-4 often tokenize at the **byte level**, meaning they operate on raw text byte sequences. This enables them to handle:\n",
    "\n",
    "* Misspellings\n",
    "* Emojis\n",
    "* Non-English scripts\n",
    "* Special characters\n",
    "\n",
    "**Illustration:**\n",
    "The string `\"Hello 👋\"` gets tokenized as a combination of regular text tokens and byte representations for the emoji.\n",
    "\n",
    "---\n",
    "\n",
    "### **Token IDs and Embeddings:**\n",
    "\n",
    "After tokenization, each token is mapped to a unique **token ID**, which corresponds to a row in the model's **embedding matrix**.\n",
    "\n",
    "Formally, given:\n",
    "\n",
    "* A vocabulary $V$ of size $|V|$\n",
    "* An embedding matrix $E \\in \\mathbb{R}^{|V| \\times d}$ where $d$ is the embedding dimension\n",
    "\n",
    "Each token $t_i$ is mapped to an ID $id_i$, and the model retrieves:\n",
    "\n",
    "$$\n",
    "\\mathbf{e}_i = E[id_i]\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{e}_i$ is the vector representation of token $t_i$.\n",
    "\n",
    "These embeddings are the numerical inputs to the Transformer model, which we'll explore in the next part.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Key Technical Points:**\n",
    "\n",
    "✔ Tokenization converts raw text into discrete, machine-readable units (tokens).\n",
    "✔ Modern LLMs use subword or byte-level tokenization for flexibility and efficiency.\n",
    "✔ The tokenization process produces token IDs that map directly to learned embeddings.\n",
    "✔ The choice of tokenizer affects a model's ability to generalize to rare, novel, or non-standard text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174af95e-2822-4bc0-9613-942a4de72b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
