{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2be000-3472-4313-b072-e0cc31b0890e",
   "metadata": {},
   "source": [
    "## Section 2, Part 5: Structuring LLM Outputs with Parsers & Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19250fe-89e0-45e6-ac83-18225016fe27",
   "metadata": {},
   "source": [
    "Hello everyone, and welcome back\\! In our last session, we did some fantastic work building our first real chains using the LangChain Expression Language (LCEL) and the `Runnable` interface. We successfully chained together prompts and models, and you saw how the pipe symbol (`|`) makes it incredibly intuitive to orchestrate complex flows.\n",
    "\n",
    "So far, however, all our chains have been outputting the same thing: a raw, unstructured Python string (`str`).\n",
    "\n",
    "```python\n",
    "# A typical chain from our last session\n",
    "chain = prompt | model\n",
    "response = chain.invoke({\"topic\": \"ice cream\"})\n",
    "\n",
    "# The output is just a string\n",
    "print(response)\n",
    "# \"content='Here are three fun facts about ice cream:\\n1. The tallest ice cream cone was over 9 feet tall...\\n2. It takes about 50 licks to finish a single scoop...\\n3. The most popular flavor is vanilla...'\"\n",
    "```\n",
    "\n",
    "A string is great for chatbots and simple Q\\&A, but what if you're building a real-world application? What if you need that data to populate a user interface, save to a database, or use as an argument to call another function or API? A blob of text just won't cut it.\n",
    "\n",
    "This is the fundamental challenge we're going to solve today. We need to tell the LLM not just *what* information to return, but *how* to format it. This is where **Output Parsers** come in. They are the crucial bridge between the creative, often messy world of the LLM and the predictable, structured world of our application code.\n",
    "\n",
    "-----\n",
    "\n",
    "### The Challenge of Unstructured Output & Intro to Output Parsers\n",
    "\n",
    "Think about it. If you ask an LLM to generate a user profile, you don't want this:\n",
    "\n",
    "`\"John Doe is 30 years old and is an active user.\"`\n",
    "\n",
    "For your application to use this information, you need this:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"John Doe\",\n",
    "  \"age\": 30,\n",
    "  \"is_active\": true\n",
    "}\n",
    "```\n",
    "\n",
    "This structured format is predictable. You can access `user['age']`, you know `is_active` will be a boolean, and you can reliably insert this data into a database table. Getting from the raw string to this clean, structured object is the job of an **Output Parser**.\n",
    "\n",
    "#### The Core Mechanism: A Two-Part Contract\n",
    "\n",
    "So, how does an output parser work? It's a clever two-part system. It doesn't just work on the *output*; it also helps shape the *input* (the prompt).\n",
    "\n",
    "1.  **Providing Instructions:** An output parser has a method called `get_format_instructions()`. This method generates a string that contains explicit instructions for the LLM on how to format its response. We must include these instructions in our prompt to guide the model. It's like telling a person, \"Please give me your answer, and make sure you write it down as a JSON object with the keys 'name' and 'age'.\"\n",
    "\n",
    "2.  **Parsing the Output:** After the LLM, guided by our instructions, returns a formatted string, the parser's `parse()` method takes over. It takes that raw string as input and transforms it into the desired Python data structure (like a `dict`, `list`, or a custom object). If the LLM's output doesn't conform to the format, this method will raise an error, letting us know something went wrong.\n",
    "\n",
    "Let's see this in action with some simple, built-in parsers.\n",
    "\n",
    "#### Simple Parsers: Getting Our Feet Wet\n",
    "\n",
    "Before we dive into the most powerful parsing methods, let's start with the basics to really understand the concept.\n",
    "\n",
    "##### **`CommaSeparatedListOutputParser`**\n",
    "\n",
    "Imagine you want the LLM to generate a list of brainstorming ideas. Getting back a Python `list` directly would be ideal. The `CommaSeparatedListOutputParser` does exactly this.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# 1. Create our parser\n",
    "list_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# 2. Get the format instructions\n",
    "format_instructions = list_parser.get_format_instructions()\n",
    "\n",
    "# 3. Create our prompt, now with formatting instructions\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List 5 {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "# Initialize our model (ensure you have OPENAI_API_KEY set in your environment)\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 4. Build our chain, now with the parser at the end\n",
    "chain = prompt | model | list_parser\n",
    "\n",
    "# Let's invoke it\n",
    "result = chain.invoke({\"subject\": \"programming languages for data science\"})\n",
    "\n",
    "print(result)\n",
    "# Expected Output: ['Python', 'R', 'SQL', 'Julia', 'Scala']\n",
    "# Notice this is a real Python list, not a string!\n",
    "```\n",
    "\n",
    "Let's break that down. We created the parser, got its instructions (` \"Your response should be a list of comma separated values, eg:  `foo, bar, baz`\"`), and put them into our prompt. The LLM saw these instructions and did its best to comply, outputting a string like `\"Python, R, SQL, Julia, Scala\"`. The `list_parser` then took that string and its `parse()` method split it by the comma to produce a clean Python `list`. The parser is the final link in our runnable chain.\n",
    "\n",
    "> **Key Takeaway:** An Output Parser transforms the LLM's string output into a structured Python object. It achieves this by first providing formatting instructions to the LLM within the prompt, and then parsing the resulting string.\n",
    "\n",
    "-----\n",
    "\n",
    "### Pydantic for Reliable, Type-Safe Structured Output\n",
    "\n",
    "Lists and simple JSON objects are useful, but for complex applications, we need more power. We need to define custom data structures with specific field names and, crucially, specific data types (`str`, `int`, `bool`, etc.).\n",
    "\n",
    "This is where Pydantic shines. **Pydantic** is a Python library for data validation and settings management using Python type annotations. It's the industry standard for defining data schemas in a clear, Pythonic way. While it's a standalone tool, its integration with LangChain is what makes it indispensable for building reliable RAG systems.\n",
    "\n",
    "With Pydantic, we can define our desired output structure as a simple Python class. LangChain can then use this class to both generate the formatting instructions and parse the LLM's output, automatically giving us a validated, type-safe Pydantic object.\n",
    "\n",
    "#### The Classic Method: `PydanticOutputParser`\n",
    "\n",
    "Let's walk through the original, foundational way of using Pydantic with LangChain. This will build a strong mental model for what's happening under the hood.\n",
    "\n",
    "##### **Step 1: Define the Schema with `pydantic.BaseModel`**\n",
    "\n",
    "First, you define the structure you want using a class that inherits from `pydantic.BaseModel`. Let's create a schema for a simple joke.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define our desired data structure\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup or question part of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline or answer part of the joke\")\n",
    "\n",
    "```\n",
    "\n",
    "Notice we're using type hints (`str`). This tells Pydantic that `setup` and `punchline` must be strings. We also used `Field(description=\"...\")`. This is a best practice\\! These descriptions aren't just for you; they are passed to the LLM as part of the format instructions, giving it crucial context to produce better results.\n",
    "\n",
    "##### **Step 2: Create the Parser from the Model**\n",
    "\n",
    "Next, we instantiate a `PydanticOutputParser`, feeding it our Pydantic model.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Create a parser instance from our Pydantic model\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "```\n",
    "\n",
    "##### **Step 3: Inject the Format Instructions into the Prompt**\n",
    "\n",
    "This is the critical step. We get the instructions from the parser and place them in the prompt.\n",
    "\n",
    "````python\n",
    "# Get the formatting instructions from the parser\n",
    "format_instructions = pydantic_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell the user a joke about {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# Let's see what the instructions look like:\n",
    "# print(format_instructions)\n",
    "# The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "#\n",
    "# As an example, for the schema {\"title\": \"Person\", \"description\": \"Information about a person\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The person's name\", \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"description\": \"The person's age\", \"type\": \"integer\"}}, \"required\": [\"name\", \"age\"]}\n",
    "# the object {\"name\": \"John Doe\", \"age\": 30} is a well-formatted instance of the schema. The object {\"name\": \"John Doe\", \"age\": \"30\"} is not well-formatted.\n",
    "#\n",
    "# Here is the output schema:\n",
    "# ```json\n",
    "# {\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"The setup or question part of the joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"The punchline or answer part of the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
    "# ```\n",
    "````\n",
    "\n",
    "As you can see, these instructions are incredibly detailed. They explain what JSON is, provide an example, and then give the exact JSON schema derived from our `Joke` Pydantic model. This is how we guide the LLM to success.\n",
    "\n",
    "##### **Step 4: Build and Invoke the Chain**\n",
    "\n",
    "Finally, we assemble our chain, with the parser as the last component.\n",
    "\n",
    "```python\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "chain = prompt | model | pydantic_parser\n",
    "\n",
    "response = chain.invoke({\"subject\": \"a programmer\"})\n",
    "\n",
    "# The output is no longer a string! It's our Pydantic object.\n",
    "print(response)\n",
    "# setup='Why do programmers prefer dark mode?' punchline='Because light attracts bugs.'\n",
    "\n",
    "# We can now access its fields with type safety\n",
    "print(f\"The punchline is: {response.punchline}\")\n",
    "# The punchline is: Because light attracts bugs.\n",
    "```\n",
    "\n",
    "Success\\! The chain's final output is a `Joke` object, which our code can now work with in a structured way.\n",
    "\n",
    "#### The Modern Approach: `.with_structured_output()`\n",
    "\n",
    "The classic method is great for understanding the process, but LangChain has evolved to make this even simpler. You've already learned that Runnables have helpful methods, and the model runnable has a particularly powerful one: `.with_structured_output()`.\n",
    "\n",
    "This method is a shortcut. It takes your Pydantic model and handles the creation of the parser and the injection of format instructions for you, all behind the scenes.\n",
    "\n",
    "##### **Side-by-Side Comparison**\n",
    "\n",
    "Let's rebuild our joke chain using this modern, recommended approach and see how much cleaner it is.\n",
    "\n",
    "```python\n",
    "# Assume 'Joke' Pydantic model and 'ChatOpenAI' model are already defined\n",
    "\n",
    "# --- Classic Method ---\n",
    "# pydantic_parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "#\n",
    "# prompt_classic = PromptTemplate(\n",
    "#     template=\"Tell the user a joke about {subject}.\\n{format_instructions}\",\n",
    "#     input_variables=[\"subject\"],\n",
    "#     partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
    "# )\n",
    "#\n",
    "# chain_classic = prompt_classic | model | pydantic_parser\n",
    "# response_classic = chain_classic.invoke({\"subject\": \"a cat\"})\n",
    "# print(f\"Classic Method Response: {response_classic}\")\n",
    "\n",
    "\n",
    "# --- Modern .with_structured_output() Method ---\n",
    "prompt_modern = PromptTemplate.from_template(\"Tell the user a joke about {subject}.\")\n",
    "\n",
    "# Chain the prompt directly to a model configured for structured output\n",
    "structured_llm = model.with_structured_output(Joke)\n",
    "chain_modern = prompt_modern | structured_llm\n",
    "\n",
    "response_modern = chain_modern.invoke({\"subject\": \"a cat\"})\n",
    "print(f\"Modern Method Response: {response_modern}\")\n",
    "# Modern Method Response: setup='Why was the cat sitting on the computer?' punchline='To keep an eye on the mouse!'\n",
    "```\n",
    "\n",
    "Look at that\\! The modern approach is far more concise. We don't need to manually create a parser instance or inject format instructions. We simply define our prompt and then pipe it into the model that we've \"pre-configured\" to produce a specific structure. This is the preferred method for new projects.\n",
    "\n",
    "> **Key Takeaway:** The `.with_structured_output(YourPydanticModel)` method is the recommended, modern way to get structured output from LLMs. It simplifies your chain by automatically handling format instruction injection and parsing.\n",
    "\n",
    "-----\n",
    "\n",
    "### Advanced Techniques & Best Practices\n",
    "\n",
    "Now that you have the fundamentals down, let's explore some more powerful techniques.\n",
    "\n",
    "#### Nested Pydantic Models\n",
    "\n",
    "Real-world data is rarely flat. You often need to extract objects that contain other objects, or lists of objects. Pydantic handles this beautifully. You just define your Pydantic models and nest them.\n",
    "\n",
    "Let's say we want to extract key information from a news article. An article has a title and a summary, but it also has a list of key takeaways, and each takeaway has its own structure.\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "\n",
    "# Define the inner, nested object first\n",
    "class KeyTakeaway(BaseModel):\n",
    "    point: str = Field(description=\"A single, crucial point from the article.\")\n",
    "    elaboration: str = Field(description=\"A brief elaboration on the point.\")\n",
    "\n",
    "# Define the main, parent object\n",
    "class ArticleSummary(BaseModel):\n",
    "    title: str = Field(description=\"The main title of the article.\")\n",
    "    summary: str = Field(description=\"A concise summary of the article's content.\")\n",
    "    takeaways: List[KeyTakeaway] = Field(description=\"A list of the most important takeaways.\")\n",
    "\n",
    "\n",
    "# Now we can use this complex model with our chain\n",
    "structured_llm = model.with_structured_output(ArticleSummary)\n",
    "\n",
    "article_text = \"\"\"\n",
    "LangChain has announced a new feature called .with_structured_output(), which promises to simplify the process of getting structured data from LLMs. \n",
    "Previously, developers had to manually create a PydanticOutputParser and inject format instructions into their prompts. \n",
    "This new method automates that process, reducing boilerplate code. The main benefit is improved developer experience and more readable code.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Extract the key information from the following article text into the ArticleSummary format.\\n\\nArticle: {text}\"\n",
    ")\n",
    "\n",
    "chain = prompt | structured_llm\n",
    "response = chain.invoke({\"text\": article_text})\n",
    "\n",
    "import json\n",
    "print(json.dumps(response.dict(), indent=2))\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"title\": \"LangChain's New .with_structured_output() Feature\",\n",
    "  \"summary\": \"LangChain has introduced a new feature, .with_structured_output(), to streamline obtaining structured data from LLMs. It automates the previously manual process of using a PydanticOutputParser and injecting format instructions, leading to less boilerplate and better code readability.\",\n",
    "  \"takeaways\": [\n",
    "    {\n",
    "      \"point\": \"Automation of Structured Output\",\n",
    "      \"elaboration\": \"The new method automates the creation of parsers and the injection of formatting instructions.\"\n",
    "    },\n",
    "    {\n",
    "      \"point\": \"Reduced Boilerplate Code\",\n",
    "      \"elaboration\": \"Developers no longer need to write manual setup code for Pydantic parsers.\"\n",
    "    },\n",
    "    {\n",
    "      \"point\": \"Improved Developer Experience\",\n",
    "      \"elaboration\": \"The simplification of the process leads to more readable and maintainable code.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "This is incredibly powerful. The LLM correctly identified the title, wrote a summary, and extracted three key takeaways as a list of structured objects, all based on our nested Pydantic schema.\n",
    "\n",
    "#### What Happens When Things Go Wrong? `OutputFixingParser`\n",
    "\n",
    "Sometimes, despite our best efforts with clear instructions, an LLM might produce a response that is slightly malformed and doesn't perfectly match our Pydantic schema. This could be due to a missing closing bracket, an extra comma, or some other hallucinated artifact. When this happens, the standard parser will raise a `ParseException`.\n",
    "\n",
    "So, what do you do? Your first step should always be to improve your prompt. Can you make it clearer? Can you use better `Field` descriptions? Can you provide an example of a good output?\n",
    "\n",
    "If prompt engineering isn't enough, LangChain provides a fallback: the `OutputFixingParser`.\n",
    "\n",
    "The `OutputFixingParser` is a special parser that wraps another parser (like our `PydanticOutputParser`). If the primary parser fails, the `OutputFixingParser` doesn't just crash. Instead, it catches the error, takes the malformed output, and feeds it *back* to the LLM in a new prompt, asking the LLM to fix its own mistake.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "# Let's imagine the model produced a malformed string\n",
    "malformed_output = '{\"setup\": \"Why did the scarecrow win an award?\", \"punchline\": \"Because he was outstanding in his field!,,}\"' # Extra comma and brace\n",
    "\n",
    "# The standard parser would fail\n",
    "# pydantic_parser.parse(malformed_output) # --> This would raise a ParseException\n",
    "\n",
    "# But we can build a recovery mechanism\n",
    "# Note: The OutputFixingParser needs a ChatModel to work\n",
    "output_fixing_parser = OutputFixingParser.from_llm(\n",
    "    parser=pydantic_parser,\n",
    "    llm=ChatOpenAI(temperature=0)\n",
    ")\n",
    "\n",
    "fixed_output = output_fixing_parser.parse(malformed_output)\n",
    "\n",
    "print(fixed_output)\n",
    "# setup='Why did the scarecrow win an award?' punchline='Because he was outstanding in his field!'\n",
    "```\n",
    "\n",
    "Think of the `OutputFixingParser` as an automated retry mechanism. It adds resilience to your chain, but it comes at the cost of an extra LLM call, so it should be used as a safety net, not a primary strategy. Your primary strategy should always be excellent prompt engineering.\n",
    "\n",
    "-----\n",
    "\n",
    "### Conclusion & Exercises\n",
    "\n",
    "Today, we've tackled one of the most important and practical aspects of building LLM applications: getting reliable, structured output. You learned:\n",
    "\n",
    "  * **Why we need parsers:** To turn raw LLM text into usable data for our applications.\n",
    "  * **The parser contract:** Parsers provide format instructions for the prompt and parse the model's output.\n",
    "  * **Pydantic is the standard:** Using `pydantic.BaseModel` is the best way to define a desired output schema.\n",
    "  * **`.with_structured_output()` is the way:** This modern LangChain method is the most efficient and readable way to build chains that produce structured data.\n",
    "  * **Handling complexity:** You can handle nested data and lists of objects simply by nesting your Pydantic models.\n",
    "  * **Building resilience:** The `OutputFixingParser` can be used as a fallback to automatically correct malformed outputs.\n",
    "\n",
    "You are now equipped to build applications that don't just talk, but that can power databases, drive UIs, and interact with other systems.\n",
    "\n",
    "#### Thought Experiments & Exercises\n",
    "\n",
    "To solidify your understanding, try working through these challenges:\n",
    "\n",
    "1.  **Recipe Extractor:** Create a Pydantic schema named `Recipe`. It should have `ingredients` (a list of strings) and `steps` (also a list of strings). Write a chain that can take a block of text describing how to bake a cake and extract this structured data.\n",
    "2.  **User Profile Enhancement:** Take the simple `UserProfile` schema (`name: str`, `age: int`). How would you modify it to include a `mailing_address`? The address itself should be a structured object with fields like `street`, `city`, and `zip_code`. (Hint: This requires a nested Pydantic model).\n",
    "3.  **API Argument Generator:** Imagine you have a function `search_flights(origin: str, destination: str, departure_date: str)`. Create a Pydantic model that matches these arguments. Then, build a chain that can take a natural language query like \"I want to fly from New York to London tomorrow\" and output a Pydantic object that could be used to call that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626d339-d7e3-47da-b5e8-8747817c907e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
