{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12700f5b-3e9c-461a-b9bb-b3ae14774051",
   "metadata": {},
   "source": [
    "# **Level 3: The Archives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f533b-9d65-4aeb-afb3-fc34c0209dbf",
   "metadata": {},
   "source": [
    "## **Part 1: Indexing – Organizing Knowledge for Your AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf4733-5a4f-413e-a7f6-6f17c5180d1f",
   "metadata": {},
   "source": [
    "\n",
    "Welcome back\\! In our previous sessions, we explored the incredible power of Large Language Models and learned how to communicate with them effectively using LangChain, prompt templates, and output parsers. We've essentially learned how to have a structured conversation with a brilliant, general-purpose mind.\n",
    "\n",
    "But what happens when we need our AI to be an expert in a specific domain? What if we need it to answer questions about our company's internal documents, a new medical research paper, or the product catalog for our e-commerce store? An LLM's pre-trained knowledge is vast, but it's not infinite, and it's certainly not up-to-the-minute. It doesn't know *your* specific data.\n",
    "\n",
    "To solve this, we need to give our AI its own library, its own knowledge base. We'll call this **\"The Archives.\"** This section is all about building and managing these archives. Our first step isn't to just dump a pile of books on the floor; it's to create a system to organize them. This process is called **Indexing**.\n",
    "\n",
    "-----\n",
    "\n",
    "## **1. What is Indexing? The Foundation of Knowledge**\n",
    "\n",
    "At its core, the concept is incredibly simple.\n",
    "\n",
    "**Indexing is the process of creating a structured reference system for your data so that it can be found quickly and efficiently later.**\n",
    "\n",
    "That's it. It’s not the act of searching, but the crucial preparation work you do *before* any searching happens.\n",
    "\n",
    "### **The Library Analogy**\n",
    "\n",
    "Imagine you have thousands of books. If you just stack them in a massive, unorganized pile, finding a specific piece of information—say, the history of the Benin Kingdom—would be a nightmare. You'd have to pick up every single book and flip through every single page. This is slow, inefficient, and frustrating.\n",
    "\n",
    "Now, think of a well-organized library. A librarian doesn't just throw books on shelves randomly. They **index** them. They might use a system like the Dewey Decimal Classification, create sections for \"History,\" \"Science,\" and \"Fiction,\" and label each shelf. When you come looking for that history book, the librarian (our future \"Retriever\") can use this index to go directly to the \"History\" section, find the right shelf, and pull the book for you in moments.\n",
    "\n",
    "**Indexing is the work the librarian does to organize the library. Retrieval is when the librarian uses that organization to find a book.** We are focusing on the first part today.\n",
    "\n",
    "> **Key Takeaway**\n",
    ">\n",
    ">   * **Indexing:** The *upfront process* of organizing and structuring your knowledge. Think of it as creating a map or a table of contents for your data.\n",
    ">   * **Retrieval:** The *real-time process* of using that index to find relevant information in response to a query.\n",
    ">\n",
    "> You cannot have efficient retrieval without good indexing first.\n",
    "\n",
    "-----\n",
    "\n",
    "## **2. Why Do We Need Indexes in AI Applications?**\n",
    "\n",
    "You might be thinking, \"Can't I just send my whole document to the LLM in the prompt?\" You could try, but you'd run into serious problems very quickly. Let's look at the \"why.\"\n",
    "\n",
    "### **The Problem: The Unorganized Data Pile**\n",
    "\n",
    "Consider these common scenarios:\n",
    "\n",
    "  * You want to build a chatbot that can answer employee questions based on a 100-page company policy PDF.\n",
    "  * You want an AI assistant that can summarize the latest news articles from five different websites every morning.\n",
    "  * You want to create a personalized study tool that can quiz you on your own lecture notes.\n",
    "\n",
    "Without an index, your application would have to:\n",
    "\n",
    "1.  Take the user's question (e.g., \"What is the company's policy on remote work?\").\n",
    "2.  Stuff the *entire* 100-page PDF along with the question into a single prompt for the LLM.\n",
    "\n",
    "This approach fails for several critical reasons:\n",
    "\n",
    "  * **Slowness & Inefficiency:** Forcing the LLM to read and process a massive wall of text for every single query is computationally expensive and incredibly slow. Your users won't wait 30 seconds for an answer.\n",
    "  * **Cost:** LLM providers charge based on the number of tokens you send and receive. Sending a huge document with every query is like paying for a long-distance call every time you want to ask a one-sentence question. It's financially unsustainable.\n",
    "  * **Context Window Limits:** Every LLM has a maximum \"context window\"—a limit on the number of tokens it can handle in a single prompt. A 100-page document will almost certainly exceed this limit, making it impossible to process.\n",
    "  * **Accuracy and Hallucinations:** When faced with too much irrelevant information (or if the relevant information can't be passed due to context limits), an LLM is more likely to get confused, miss the right answer, or worse, **hallucinate**—invent a plausible but incorrect answer.\n",
    "\n",
    "### **The Solution: The Indexed Archive**\n",
    "\n",
    "By indexing our data first, we change the entire workflow. Instead of sending the whole library, we use the user's query to quickly find the most relevant \"book\" or \"chapter\" in our indexed archive and only send that small, relevant snippet to the LLM.\n",
    "\n",
    "This leads to massive benefits:\n",
    "\n",
    "  * **Speed:** Looking up information in an index is lightning fast.\n",
    "  * **Scalability:** An indexed system can handle millions of documents without slowing down.\n",
    "  * **Cost-Effectiveness:** We send only small, relevant chunks of text to the LLM, drastically reducing token usage and cost.\n",
    "  * **Accuracy:** By providing the LLM with focused, relevant context, we empower it to give accurate, fact-based answers and significantly reduce the chance of hallucination.\n",
    "\n",
    "-----\n",
    "\n",
    "## **3. Types of Indexes: A Conceptual Overview**\n",
    "\n",
    "There are many ways to organize information, from simple to highly sophisticated. For now, we'll introduce the main concepts to build your mental model.\n",
    "\n",
    "  * **Keyword Indexes:** This is the most traditional form of indexing, similar to the index at the back of a textbook or a basic search engine. It creates a map of which documents contain which specific words. If you search for \"policy,\" it finds every document that explicitly contains the word \"policy.\" It's fast and simple but has a major weakness: it doesn't understand meaning. A search for \"rules for working from home\" would fail if the document only used the phrase \"remote work policy.\"\n",
    "\n",
    "  * **Vector Indexes:** This is the powerhouse of modern RAG systems, and we will dedicate our entire next session to it. Instead of mapping keywords, a vector index maps the *semantic meaning* of your text. It understands that \"rules for working from home\" and \"remote work policy\" are conceptually similar. This allows for incredibly powerful and nuanced searching based on ideas, not just words. **We'll get to this soon, so just keep the concept in mind for now.**\n",
    "\n",
    "  * **Structured Indexes:** This is our focus for today because it builds directly on skills you already have\\! A structured index is one where we organize our data into a predefined schema with specific fields. Think of a spreadsheet or a database table.\n",
    "\n",
    "Remember your work with **Pydantic** and **Output Parsers**? You learned how to define a data structure (like a `User` or `Product` class) and then parse information into that structure. This is the exact skill we use to create a structured index. We decide what our \"knowledge records\" should look like and then store them in that format.\n",
    "\n",
    "-----\n",
    "\n",
    "## **4. Practical Example: Building a Simple Structured Index with Pydantic**\n",
    "\n",
    "Let's make this real. Imagine we're building a simple Q\\&A bot for a software company's support page. We have a bunch of raw text containing frequently asked questions and their answers. Our goal is to index this information so we can easily search for it.\n",
    "\n",
    "### **Step 1: Define the Structure of our Knowledge (Data Definition)**\n",
    "\n",
    "First, we need to decide what a single piece of knowledge—one FAQ—looks like. It has a question, an answer, and maybe a category. This is a perfect job for a Pydantic `BaseModel`\\!\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Define the structure for a single FAQ item.\n",
    "# This acts as the schema for our 'knowledge records'.\n",
    "class FAQ(BaseModel):\n",
    "    \"\"\"Represents a single Frequently Asked Question.\"\"\"\n",
    "    question: str = Field(description=\"The question being asked.\")\n",
    "    answer: str = Field(description=\"The corresponding answer.\")\n",
    "    category: str = Field(description=\"The category the FAQ belongs to, e.g., 'Billing', 'Account', 'Technical'.\")\n",
    "\n",
    "print(\"Pydantic 'FAQ' model defined successfully.\")\n",
    "print(FAQ.schema_json(indent=2))\n",
    "```\n",
    "\n",
    "By defining this `FAQ` model, we have created the \"shelf label\" for our library. We've decided that every piece of information we store must fit this structure.\n",
    "\n",
    "### **Step 2: Parse Raw Data into Our Structure**\n",
    "\n",
    "Now, let's say we have our raw data as a list of simple strings. In a real-world project, this data might come from a text file, a website scrape, or a database.\n",
    "\n",
    "```python\n",
    "# Raw, unstructured data we need to index.\n",
    "# In a real app, this might come from a file or API.\n",
    "raw_faq_data = [\n",
    "    \"Question: How do I reset my password? Answer: You can reset your password by going to the 'Account' page and clicking 'Forgot Password'. Category: Account.\",\n",
    "    \"Question: What payment methods do you accept? Answer: We accept all major credit cards and PayPal. Category: Billing.\",\n",
    "    \"Question: My software is running slow, what should I do? Answer: Try clearing your cache and restarting the application. If the problem persists, contact support. Category: Technical.\"\n",
    "]\n",
    "```\n",
    "\n",
    "How do we get this raw text into our beautiful `FAQ` Pydantic objects? You already know how\\! In Section 2, you learned about **Output Parsers**. We could set up a prompt telling an LLM to extract the information and format it according to our `PydanticOutputParser` for the `FAQ` model.\n",
    "\n",
    "For today's lesson, we'll simulate that parsing step and create the objects directly to focus on the indexing part.\n",
    "\n",
    "```python\n",
    "# Manually creating the structured objects for this example.\n",
    "# In a real RAG system, you would use an LLM with an Output Parser here.\n",
    "faq_objects = [\n",
    "    FAQ(question=\"How do I reset my password?\", answer=\"You can reset your password by going to the 'Account' page and clicking 'Forgot Password'.\", category=\"Account\"),\n",
    "    FAQ(question=\"What payment methods do you accept?\", answer=\"We accept all major credit cards and PayPal.\", category=\"Billing\"),\n",
    "    FAQ(question=\"My software is running slow, what should I do?\", answer=\"Try clearing your cache and restarting the application. If the problem persists, contact support.\", category=\"Technical\")\n",
    "]\n",
    "\n",
    "print(\"Successfully created a list of structured FAQ objects.\")\n",
    "```\n",
    "\n",
    "### **Step 3: Store the Objects in a Simple Index**\n",
    "\n",
    "Now for the \"indexing\" itself. For our simple case, our index can just be a Python list that holds all of our structured `FAQ` objects. This is an **in-memory index**—it exists only while our script is running.\n",
    "\n",
    "```python\n",
    "# Our simple, in-memory structured index.\n",
    "# It's just a list of our Pydantic objects.\n",
    "faq_index: List[FAQ] = faq_objects\n",
    "\n",
    "print(f\"Index created. It contains {len(faq_index)} items.\")\n",
    "```\n",
    "\n",
    "That's it\\! We have successfully indexed our data. We've taken raw information, given it a robust structure, and stored it in an organized way.\n",
    "\n",
    "### **Step 4: Simulating a Lookup (Basic Retrieval)**\n",
    "\n",
    "Let's see why this is useful. We can now write a simple search function that \"retrieves\" information from our index. Our first version will be a basic keyword search.\n",
    "\n",
    "```python\n",
    "def find_faq_by_keyword(query: str, index: List[FAQ]) -> List[FAQ]:\n",
    "    \"\"\"\n",
    "    A simple search function to find FAQs containing a keyword in their question.\n",
    "    This simulates a basic retrieval step.\n",
    "    \"\"\"\n",
    "    query = query.lower()\n",
    "    results = []\n",
    "    for faq_item in index:\n",
    "        # A very basic search: check if the query keyword is in the question.\n",
    "        if query in faq_item.question.lower():\n",
    "            results.append(faq_item)\n",
    "    return results\n",
    "\n",
    "# Let's test our search function\n",
    "user_query = \"password\"\n",
    "search_results = find_faq_by_keyword(user_query, faq_index)\n",
    "\n",
    "if search_results:\n",
    "    print(f\"Found {len(search_results)} result(s) for the query: '{user_query}'\")\n",
    "    for result in search_results:\n",
    "        print(f\"\\nQ: {result.question}\")\n",
    "        print(f\"A: {result.answer}\")\n",
    "        print(f\"Category: {result.category}\")\n",
    "else:\n",
    "    print(f\"No results found for the query: '{user_query}'\")\n",
    "```\n",
    "\n",
    "Look at that\\! We queried our index and got an immediate, relevant result. We can now take this specific answer and provide it to the user or feed it into an LLM as context, saying, \"Using the following information, answer the user's question about their password.\"\n",
    "\n",
    "This is far more efficient than sending all the FAQs to the LLM at once.\n",
    "\n",
    "-----\n",
    "\n",
    "## **5. The Indexing Workflow for RAG (Connecting the Dots)**\n",
    "\n",
    "Let's zoom out and see where today's lesson fits into the bigger picture of a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "(Raw Data) -\\> (Document Loading) -\\> (Unstructured Text) -\\> (Text Splitting) -\\> (Chunks of Text) -\\> **[Parse into Structured Format (Pydantic)]** -\\> **[Store Structured Objects (Our Index)]**\n",
    "\n",
    "  * **Raw Data:** This is your source material—PDFs, websites, text files, etc.\n",
    "  * **Document Loading & Text Splitting:** These are preliminary steps (which we will cover later) to load the data into your program and break it into manageable chunks. Don't worry about the details here yet.\n",
    "  * **Parse into Structured Format:** This is what we focused on today—using our Pydantic models as a schema to structure the raw text chunks.\n",
    "  * **Store Structured Objects:** This is the final indexing step for today—placing those structured objects into a searchable collection, like our Python list.\n",
    "\n",
    "We've tackled the core logic of turning messy data into an organized, queryable archive.\n",
    "\n",
    "-----\n",
    "\n",
    "## **6. Troubleshooting & Common Pitfalls**\n",
    "\n",
    "As you build your own indexes, you'll encounter a few common issues.\n",
    "\n",
    "  * **Incorrect Pydantic Schema:** What if your data has a field your `BaseModel` doesn't? Or vice versa? The parsing step will fail. This is why defining a good, accurate schema that truly represents your data is the most important first step. **Your schema is the blueprint for your archive.**\n",
    "  * **Poor Data Quality:** The classic saying, \"garbage in, garbage out,\" is paramount here. If your source documents are full of typos, formatting errors, or incorrect information, your index will also be low-quality. A good index cannot fix bad data. Always preprocess and clean your data as much as possible.\n",
    "  * **Scalability of Our Simple Index:** Our `find_faq_by_keyword` function works fine for 3 items, or even 300. But what about 3 million? Iterating through a Python list every time would become slow again. This simple list-based index is fantastic for learning and small projects, but it does not scale.\n",
    "\n",
    "> **Pro Tip**\n",
    ">\n",
    "> Spend most of your time on data definition and cleaning. A well-designed Pydantic model and clean source data will save you countless hours of debugging later. This upfront investment in quality is the hallmark of a professional AI/ML engineer.\n",
    "\n",
    "-----\n",
    "\n",
    "Of course. Here is the revised final section.\n",
    "\n",
    "***\n",
    "\n",
    "## **7. Looking Ahead: From Simple Records to Intelligent Archives**\n",
    "\n",
    "We've successfully built a clean, structured index for well-defined data like FAQs. This is a fantastic foundation and a crucial skill for handling any data that fits a predictable pattern.\n",
    "\n",
    "But what happens when our source isn't a neat list? What if it's a 50-page PDF, a collection of long articles, or transcripts from meetings? We can't just stuff an entire book into a single `question` field. Our simple keyword search also remains a major limitation; it's fast, but it's not \"smart.\"\n",
    "\n",
    "This brings us to the next logical steps for building a truly powerful archive:\n",
    "\n",
    "1.  **Chunking:** Before we can index large, unstructured documents, we must first break them down into smaller, digestible, and meaningful pieces. This process is called **chunking**. Think of it as intelligently splitting a book into coherent paragraphs or sections that we can then index individually.\n",
    "\n",
    "2.  **Vector Embeddings:** Once we have our text chunks, how do we search them by *meaning* instead of just keywords? To solve this, we'll convert each chunk into a rich numerical representation—a vector—that captures its semantic essence. This process, creating **vector embeddings**, is what allows us to find a chunk about a \"remote work policy\" even when a user asks about \"rules for working from home.\"\n",
    "\n",
    "3.  **Vector Stores:** And where do we keep these powerful vectors? A Python list won't scale to millions of documents. We'll need a specialized database built for incredibly fast and efficient meaning-based searches. This is the role of a **Vector Store**.\n",
    "\n",
    "Our path forward is clear. In the upcoming sessions, we will master the art of **chunking**, learn how to transform text into powerful **vector embeddings**, and finally, store it all in a scalable **vector store**. Get ready to upgrade \"The Archives\" from a simple bookshelf into an intelligent, semantic library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154c3a4-be5e-4559-91c1-a0d7329fbf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
