{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2e58b9-4d69-4a73-9641-5261ff8ac10c",
   "metadata": {},
   "source": [
    "# **Level 3: The Archives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6655b2-4691-45a9-b12f-fb493bb24586",
   "metadata": {},
   "source": [
    "## **Part 2: Document Loading – Getting Your Knowledge into LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28107f31-cd58-4b57-b920-e27a8d0c8b42",
   "metadata": {},
   "source": [
    "Hello everyone, and welcome back\\! In our last session, we introduced the concept of \"The Archives\"—our AI's knowledge base. We talked about \"Indexing,\" which is the crucial process of organizing that knowledge so our system can find what it needs efficiently. We even looked at how we could use Pydantic to enforce a specific, structured format for our indexed data.\n",
    "\n",
    "But this raises a fundamental question: Before we can organize, structure, or index any information, we first have to *get* it. Our company's data isn't just floating in the ether; it lives in specific places. How do we bring our internal wikis, our product manuals, our research papers, or our customer support chat logs *into* our AI application in the first place?\n",
    "\n",
    "That's precisely what we're covering today. This is the very first practical step in building any RAG system: **Document Loading**.\n",
    "\n",
    "-----\n",
    "\n",
    "### **What is Document Loading? (The Gateway to Your Data)**\n",
    "\n",
    "Let's start with a simple, clear definition.\n",
    "\n",
    "**Document loading** is the process of reading data from various sources—like files on your computer, pages on a website, or entries in a database—and converting it into a standardized format that LangChain can understand and work with.\n",
    "\n",
    "Think of it as the librarian at the entrance of the archives. They don't just throw raw books onto the shelves. They first take each item, whether it's a book, a magazine, or a scroll, and process it into a standard format: a library book with a catalog card. Document loading is our digital librarian.\n",
    "\n",
    "#### **The \"Standardized Format\": The LangChain `Document` Object**\n",
    "\n",
    "So, what is this \"standardized format\" I keep mentioning? In LangChain, it's a wonderfully simple yet powerful Python object called a **`Document`**.\n",
    "\n",
    "A LangChain `Document` is the universal container for a piece of text that you load into your application. No matter where your data comes from—a PDF, a `.txt` file, a website—the loader's job is to stuff it into one or more of these `Document` objects.\n",
    "\n",
    "Each `Document` object has two core components:\n",
    "\n",
    "1.  **`page_content`**: A string (`str`) that holds the actual text content of the data. This is the \"what\"—the substance of your knowledge.\n",
    "2.  **`metadata`**: A Python dictionary (`dict`) that holds additional information *about* the content. This is the \"where, who, and when.\" It can include things like the source filename, the page number, the author, the creation date, or a URL.\n",
    "\n",
    "Let's make this concrete with an analogy. Imagine you're collecting recipes.\n",
    "\n",
    "  * A recipe clipped from a magazine.\n",
    "  * A handwritten recipe from your grandmother.\n",
    "  * A recipe saved from a food blog.\n",
    "\n",
    "To organize them, you decide to type each one up on a standard index card.\n",
    "\n",
    "  * The **`page_content`** would be the recipe itself: the ingredients and the instructions.\n",
    "  * The **`metadata`** would be what you write at the top of the card: `{\"source\": \"Good Food Magazine\", \"date\": \"June 2023\"}` or `{\"source\": \"Grandma's Kitchen\", \"author\": \"Grandma Sue\"}` or `{\"source\": \"AllRecipes.com\", \"url\": \"http://...\"}`.\n",
    "\n",
    "This metadata is incredibly powerful. Down the line, it allows us to filter our searches (\"only show me recipes from Grandma\") or to cite our sources when our AI gives an answer (\"I found this information in the Q3\\_Financials.pdf on page 4\").\n",
    "\n",
    "> **Key Takeaway:** The `Document` object, with its `page_content` and `metadata`, is LangChain's universal language for data. Document Loaders are the translators that convert raw files into this universal language. This standardization is what allows all the different parts of LangChain to work together seamlessly.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Why Do We Need Dedicated Document Loaders?**\n",
    "\n",
    "You might be thinking, \"Can't I just use `with open('my_file.txt', 'r') as f: ...` to read a file? Why do I need a special LangChain thing for this?\"\n",
    "\n",
    "That's a great question, and it works perfectly... if all your data for the rest of your career lives in simple `.txt` files. But in the real world, data is messy and diverse. It lives in:\n",
    "\n",
    "  * PDFs with complex layouts, tables, and images.\n",
    "  * Microsoft Word documents.\n",
    "  * Web pages with HTML tags, navigation bars, and ads.\n",
    "  * Structured CSV or JSON files.\n",
    "  * Databases that require specific connection credentials.\n",
    "  * APIs that need authentication keys.\n",
    "  * Collaboration tools like Notion, Confluence, or Slack.\n",
    "\n",
    "Each of these sources requires a different method to extract the clean text content. Parsing a PDF is fundamentally different from scraping a website. This is where LangChain's `DocumentLoaders` become our best friends.\n",
    "\n",
    "**LangChain's solution is to provide a vast ecosystem of pre-built `DocumentLoader`s that handle this complexity for us.** They abstract away the messy, source-specific logic. All we need to do is pick the right loader for our source, point it at the data, and it does the hard work of parsing and converting it into those clean `Document` objects we just discussed.\n",
    "\n",
    "This saves us an enormous amount of time and effort, freeing us from writing custom, brittle parsing code for every new data type we encounter. Some loaders even handle authentication for password-protected or private sources, though we won't dive deep into that today.\n",
    "\n",
    "-----\n",
    "\n",
    "### **How Document Loaders Work (A Simplified Look Under the Hood)**\n",
    "\n",
    "The beauty of LangChain's design is its consistency. While the internal logic of each loader is unique, the way we *use* them is almost always the same.\n",
    "\n",
    "Nearly every `DocumentLoader` you encounter will have a primary method called **`load()`**.\n",
    "\n",
    "The flow is simple:\n",
    "\n",
    "1.  **Instantiate the Loader:** You create an instance of the specific loader you need, giving it a path to your data (e.g., a file path or a URL).\n",
    "2.  **Call `.load()`:** You call the `load()` method on that instance.\n",
    "3.  **Receive Documents:** The method returns a list of `Document` objects (`List[Document]`).\n",
    "\n",
    "That's it\\!\n",
    "\n",
    "`Point Loader at Source -> Loader Reads & Parses -> Loader Creates Documents -> Returns a List[Document]`\n",
    "\n",
    "You might wonder, \"Why a list of documents? Why not just one?\" Sometimes, a single source file naturally breaks into multiple documents. For example, a `PyPDFLoader` will often return one `Document` object *per page* of the PDF. A `CSVLoader` might return one `Document` per row. This is a design choice by the loader's author to create logical initial separations of the content.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Common Document Loaders: Your First Practical Tools**\n",
    "\n",
    "Let's get our hands dirty. LangChain supports over 100 different loaders, but you'll find yourself using a core few for most of your projects. We'll focus on three essential ones today.\n",
    "\n",
    "**A quick prerequisite:** Most loaders are not installed with the core `langchain` library. They have extra dependencies that you need to install yourself. This keeps the core library lightweight.\n",
    "\n",
    "#### **1. `TextLoader`: Your Simplest Friend**\n",
    "\n",
    "This is the most basic loader, designed for plain `.txt` files.\n",
    "\n",
    "First, let's create a dummy text file to work with. In a real project, this would be a file on your disk. For this lecture, we'll just write it programmatically.\n",
    "\n",
    "```python\n",
    "# Create a dummy text file for our example\n",
    "with open(\"my_first_document.txt\", \"w\") as f:\n",
    "    f.write(\"This is the first sentence of our document.\\n\")\n",
    "    f.write(\"Here is a second line with more information.\\n\")\n",
    "    f.write(\"The document loader should be able to read this all in.\\n\")\n",
    "```\n",
    "\n",
    "Now, let's load it using `TextLoader`.\n",
    "\n",
    "```python\n",
    "# TextLoader doesn't require a special installation beyond core langchain\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# 1. Instantiate the loader with the path to our file\n",
    "loader = TextLoader(\"my_first_document.txt\")\n",
    "\n",
    "# 2. Call the .load() method\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s).\")\n",
    "print(\"---\")\n",
    "\n",
    "# Inspect the first document\n",
    "doc = documents[0]\n",
    "\n",
    "print(\"Page Content:\")\n",
    "print(doc.page_content)\n",
    "print(\"\\n---\")\n",
    "\n",
    "print(\"Metadata:\")\n",
    "print(doc.metadata)\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Loaded 1 document(s).\n",
    "---\n",
    "Page Content:\n",
    "This is the first sentence of our document.\n",
    "Here is a second line with more information.\n",
    "The document loader should be able to read this all in.\n",
    "\n",
    "---\n",
    "Metadata:\n",
    "{'source': 'my_first_document.txt'}\n",
    "```\n",
    "\n",
    "Notice how it loaded the entire file into a single `Document`. And look at the metadata\\! The loader automatically added the `source` file path for us. Simple, effective, and a great starting point.\n",
    "\n",
    "#### **2. `PyPDFLoader`: Tackling PDFs**\n",
    "\n",
    "PDFs are everywhere. To handle them, we'll use `PyPDFLoader`. This one requires an external library called `pypdf`.\n",
    "\n",
    "**Installation:**\n",
    "\n",
    "```bash\n",
    "pip install pypdf\n",
    "```\n",
    "\n",
    "Let's use a simple example. For this to work, you'll need to create a PDF file named `example_report.pdf` and place it in the same directory as your code. The PDF can contain a couple of pages of simple text.\n",
    "\n",
    "*Page 1 of PDF: \"This is the executive summary on page one.\"*\n",
    "*Page 2 of PDF: \"This is the detailed analysis on page two.\"*\n",
    "\n",
    "Now, the code:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 1. Instantiate the loader with the path to the PDF\n",
    "# NOTE: You must have an 'example_report.pdf' file in the same directory\n",
    "try:\n",
    "    loader = PyPDFLoader(\"example_report.pdf\")\n",
    "\n",
    "    # 2. Call the .load() method\n",
    "    # PyPDFLoader also has a .load_and_split() which is convenient but we'll learn that later.\n",
    "    documents = loader.load()\n",
    "\n",
    "    print(f\"Loaded {len(documents)} document(s) from the PDF.\")\n",
    "    print(\"---\")\n",
    "\n",
    "    # Inspect the documents\n",
    "    # PyPDFLoader creates one Document per page\n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"--- Document {i+1} ---\")\n",
    "        print(\"Page Content Snippet:\")\n",
    "        # We print only the first 100 chars to keep the output clean\n",
    "        print(doc.page_content[:100] + \"...\")\n",
    "        print(\"\\nMetadata:\")\n",
    "        print(doc.metadata)\n",
    "        print(\"\\n\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'example_report.pdf' not found. Please create this file to run the example.\")\n",
    "\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Loaded 2 document(s) from the PDF.\n",
    "---\n",
    "--- Document 1 ---\n",
    "Page Content Snippet:\n",
    "This is the executive summary on page one....\n",
    "\n",
    "Metadata:\n",
    "{'source': 'example_report.pdf', 'page': 0}\n",
    "\n",
    "\n",
    "--- Document 2 ---\n",
    "Page Content Snippet:\n",
    "This is the detailed analysis on page two....\n",
    "\n",
    "Metadata:\n",
    "{'source': 'example_report.pdf', 'page': 1}\n",
    "```\n",
    "\n",
    "This is fantastic\\! Notice two key things:\n",
    "\n",
    "1.  It created **two `Document` objects**, one for each page. This is a very common and useful behavior.\n",
    "2.  The **metadata is richer**. In addition to the `source`, it automatically extracted and added the `page` number (programmers count from 0, so page 1 is `page: 0`). This is invaluable context\\!\n",
    "\n",
    "#### **3. `WebBaseLoader`: Ingesting the Web**\n",
    "\n",
    "What if our knowledge is on a website? `WebBaseLoader` is our tool for the job. It uses the `BeautifulSoup` library under the hood to download and parse HTML content.\n",
    "\n",
    "**Installation:**\n",
    "\n",
    "```bash\n",
    "pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "Let's try to load a simple, text-heavy webpage.\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# 1. Instantiate the loader with the URL\n",
    "# Using a simple, stable blog post as an example\n",
    "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "loader = WebBaseLoader(url)\n",
    "\n",
    "# 2. Call the .load() method\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s) from the URL.\")\n",
    "print(\"---\")\n",
    "\n",
    "# Inspect the document\n",
    "doc = documents[0]\n",
    "\n",
    "print(\"Page Content Snippet (first 500 characters):\")\n",
    "print(doc.page_content[:500] + \"...\")\n",
    "print(\"\\n---\")\n",
    "\n",
    "print(\"Metadata:\")\n",
    "print(doc.metadata)\n",
    "\n",
    "```\n",
    "\n",
    "**Expected Output (will vary slightly as the page content changes):**\n",
    "\n",
    "```\n",
    "Loaded 1 document(s) from the URL.\n",
    "---\n",
    "Page Content Snippet (first 500 characters):\n",
    "\n",
    "\n",
    "      LLM Powered Autonomous Agents | Lil'Log\n",
    "    \n",
    "\n",
    "      Lil'Log\n",
    "    \n",
    "      Posts\n",
    "      Archive\n",
    "      Notes\n",
    "      Projects\n",
    "      About\n",
    "      Newsletter\n",
    "      \n",
    "        Search\n",
    "      \n",
    "    \n",
    "    \n",
    "      LLM Powered Autonomous Agents\n",
    "    \n",
    "    Jun 23, 2023  |  20 min read\n",
    "    \n",
    "      \n",
    "      Table of Contents\n",
    "      \n",
    "    \n",
    "    \n",
    "      Component One: Planning\n",
    "      Component Two: Memory\n",
    "      Component Three: Tool Use\n",
    "      Case Studies\n",
    "      Challenges\n",
    "      Conclusion\n",
    "      References\n",
    "    \n",
    "  \n",
    "Building agents with LLM (large language model) as its core controller is a cool concept...\n",
    "---\n",
    "Metadata:\n",
    "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potential seems significant, but the path to robust intelligence is long and arduous. It is a good moment to pause, reflect on the lessons learned from these early experiments, and chart a clear path forward.\\nAgent = LLM + Planning + Memory + Tool Use…', 'language': 'en'}\n",
    "```\n",
    "\n",
    "Look at that\\! It pulled the text content from the webpage. Notice that the `page_content` can be a bit \"messy\" – it often includes leftover navigation text and other HTML artifacts. This is normal, and it's a problem we will learn to solve later.\n",
    "\n",
    "But check out the **metadata**\\! The `WebBaseLoader` is smart enough to extract the page `title`, `description`, and `language` in addition to the `source` URL. This is free, valuable context we get just by choosing the right loader.\n",
    "\n",
    "#### **Quick Mentions: The Wider Ecosystem**\n",
    "\n",
    "To give you a sense of the possibilities, here are a few other popular loaders you should be aware of:\n",
    "\n",
    "  * `CSVLoader`: For loading data from comma-separated value files.\n",
    "  * `DirectoryLoader`: A powerful loader that can load all files from a folder. You tell it which loader to use for which file type (e.g., use `PyPDFLoader` for `.pdf` files and `TextLoader` for `.txt` files).\n",
    "  * `UnstructuredHTMLLoader` / `RecursiveUrlLoader`: For more advanced and robust web scraping.\n",
    "  * And many, many more for Notion, GitHub, Confluence, Slack, Discord, Google Drive, SQL Databases, etc.\n",
    "\n",
    "> **Pro Tip:** When you have a new data source, your first step should always be to search the LangChain documentation for a pre-existing loader. Don't reinvent the wheel\\!\n",
    "\n",
    "-----\n",
    "\n",
    "### **Best Practices & Troubleshooting**\n",
    "\n",
    "When you're starting, you'll inevitably run into a few common issues. Let's head them off now.\n",
    "\n",
    "  * **File Paths are Tricky:** A `FileNotFoundError` is the most common error. Remember the difference between **relative paths** (`\"my_docs/report.pdf\"`) and **absolute paths** (`\"/Users/myname/Documents/project/my_docs/report.pdf\"`). Make sure your code is running from a location that can \"see\" the file you're pointing to.\n",
    "  * **Install Your Dependencies:** Remember `pip install pypdf`, `pip install beautifulsoup4`, etc. If you get an `ImportError`, it's almost always a missing dependency. The error message will usually tell you what to install.\n",
    "  * **Character Encoding:** Occasionally, when loading text files (`TextLoader`), you might see a `UnicodeDecodeError`. This happens when the file isn't saved in the standard `UTF-8` format. `TextLoader` has an `encoding` argument you can use to fix this, e.g., `TextLoader(\"my_file.txt\", encoding=\"latin-1\")`.\n",
    "  * **My File is HUGE\\!** What happens if you load a 500-page book? You'll get a `Document` with a massive `page_content` string. This is a problem because LLMs have a limited \"context window\" (a limit on how much text they can look at once). We can't just feed them a whole book. **This is a critical point that we will solve in our very next session.** For now, just know that loading is the first step; processing large documents is the next.\n",
    "  * **Revisit Metadata:** I cannot overstate this: **pay attention to your metadata**. Good metadata is the key to building smart, accurate, and trustworthy RAG systems. It allows for filtering, citation, and provides crucial context to the LLM.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Connecting to the \"Archives\" Workflow**\n",
    "\n",
    "Let's update our mental model and our diagram to see exactly where Document Loading fits. It's the very first step, the bridge from the outside world into LangChain.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"The Outside World\"\n",
    "        A1[PDF Files]\n",
    "        A2[Websites]\n",
    "        A3[Text Files]\n",
    "        A4[...]\n",
    "    end\n",
    "\n",
    "    subgraph \"LangChain RAG Pipeline\"\n",
    "        B{Document Loaders <br/> (PyPDFLoader, WebBaseLoader, etc.)}\n",
    "        C[List of LangChain 'Document' Objects]\n",
    "        D[Next Step: Text Splitter]\n",
    "        E{Parse into Structured Format <br/> (Pydantic/Output Parsers)}\n",
    "        F[Simple Structured Index]\n",
    "    end\n",
    "    \n",
    "    A1 --> B\n",
    "    A2 --> B\n",
    "    A3 --> B\n",
    "    A4 --> B\n",
    "    B --> C\n",
    "    C --> D\n",
    "    C --> E\n",
    "    E --> F\n",
    "\n",
    "```\n",
    "\n",
    "As the diagram shows, **Document Loaders** take all our raw data sources and transform them into the standardized list of `LangChain Documents`. From there, we can either send them for structured parsing (like we discussed in the last lecture) or, as we'll soon see, send them to a \"Text Splitter\" to be broken down into smaller pieces.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Looking Ahead: From Loaded Documents to Usable Chunks**\n",
    "\n",
    "We've successfully completed the first major step\\! We can now bring knowledge from a variety of sources into our LangChain application. We have our data neatly packaged in `Document` objects, complete with `page_content` and valuable `metadata`.\n",
    "\n",
    "But we've also identified a major challenge: these documents can be huge. The `page_content` of a loaded webpage or a 30-page PDF will be far too large to fit into a prompt for a Large Language Model.\n",
    "\n",
    "So, what's the next logical step after loading our data? How do we handle these massive documents?\n",
    "\n",
    "In our next session, we'll tackle this head-on. We will learn how to take these large `Document` objects and intelligently break them down into smaller, more manageable, and contextually relevant pieces. This essential process is called **Text Splitting**.\n",
    "\n",
    "See you then\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35968125-ebe3-487b-85d7-b94ffdff8714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
