{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcbbcf3-7c1e-4ead-8665-d744a6780ba1",
   "metadata": {},
   "source": [
    "# **Level 3: The Archives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256396c-c54e-418c-91c8-888f958317d4",
   "metadata": {},
   "source": [
    "## Part 3: Chunking – Breaking Down Knowledge for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab41bbc-73c3-4847-80f7-18acdb9888b7",
   "metadata": {},
   "source": [
    "\n",
    "Hello everyone, and welcome back\\! In our last session on **Document Loading**, we learned how to be digital librarians, taking information from all sorts of sources—PDFs, websites, text files—and loading them into our LangChain ecosystem as `Document` objects. This was a massive first step. We now have our raw knowledge inside the system.\n",
    "\n",
    "But this brings us to a critical, and very practical, problem. What happens when one of those `Document` objects is a 500-page book, a lengthy scientific paper, or a massive legal contract? Can we just hand this entire mountain of text to our Large Language Model and say, \"Hey, find the answer to my specific question in here\"?\n",
    "\n",
    "The short answer is **no**. Trying to do this is like asking someone to find a single grain of sand on a vast beach by showing them the whole beach at once. It's inefficient, often impossible, and misses the point of precision. This is where our next core skill comes in: **Chunking**.\n",
    "\n",
    "## 1\\. What is Chunking? The Art of Breaking It Down\n",
    "\n",
    "**Chunking**, or as it's more formally known in LangChain, **Text Splitting**, is the process of taking large `Document` objects and breaking them down into smaller, more manageable, and semantically coherent segments. We call these smaller pieces \"chunks.\"\n",
    "\n",
    "Think about it this way:\n",
    "\n",
    "  * **`Document Loading`** is like acquiring a whole encyclopedia set for our library.\n",
    "  * **`Chunking`** is like carefully separating that encyclopedia into individual, labeled volumes, and then breaking down those volumes into distinct entries or articles.\n",
    "\n",
    "Our goal isn't just to chop up the text randomly. The art of chunking is to create pieces that are small enough for an LLM to handle but large enough to contain a complete thought or a self-contained piece of information.\n",
    "\n",
    "> ### **Key Takeaway**\n",
    ">\n",
    "> **Chunking** is the process of dividing large text documents into smaller, coherent segments. The purpose is to make the text manageable for LLMs (due to context window limits) and more precise for retrieval. We want to find *relevant segments*, not whole books.\n",
    "\n",
    "-----\n",
    "\n",
    "## 2\\. Why Smart Chunking is So Important\n",
    "\n",
    "You might be thinking, \"Okay, breaking things down makes sense. But why is it so crucial?\" There are three fundamental reasons, and understanding them will make you much better at building effective RAG systems.\n",
    "\n",
    "### a. LLM Context Window Limits\n",
    "\n",
    "This is the most immediate and non-negotiable reason. Every LLM has a **context window**, which is a hard limit on the amount of text it can process at one time (both for the input prompt and the generated output).\n",
    "\n",
    "  * Think of it as the LLM's short-term memory. It can't read an entire book at once; it can only read a few pages.\n",
    "  * This limit is measured in **tokens**. For simplicity, you can think of a token as roughly a word or a part of a word. A model like `gpt-3.5-turbo` might have a context window of 4,096 or 16,385 tokens (roughly 3,000 to 12,000 words). `Gemini` models have even larger windows, but a limit always exists.\n",
    "\n",
    "If you send a document that exceeds this limit, you'll get an error. The model simply cannot handle it. Chunking ensures that the pieces of text we eventually send to the LLM are well within this limit.\n",
    "\n",
    "### b. Relevance & Precision for Retrieval (Foreshadowing)\n",
    "\n",
    "This is the more strategic reason. Remember, our ultimate goal in RAG is to retrieve *only the most relevant information* to answer a user's query.\n",
    "\n",
    "  * **If your chunks are too large:** Imagine a 20-page chapter that mentions our key topic on only one page. If we retrieve that whole chapter, we are forcing the LLM to sift through 19 pages of noise to find the 1 page of signal. This dilutes the relevant information and can lead to worse answers.\n",
    "  * **If your chunks are too small:** Imagine splitting the sentence \"The capital of France is Paris\" right down the middle. \"The capital of France\" and \"is Paris\" are two separate chunks. Neither chunk alone is very useful. We've lost the crucial context that connects them.\n",
    "\n",
    "This leads to the \"Goldilocks Zone\" of chunking: chunks should be **not too big, not too small, but just right**. They need to be large enough to be meaningful but small enough to be precise.\n",
    "\n",
    "### c. Cost Efficiency\n",
    "\n",
    "LLM APIs are not free. You are typically charged based on the number of tokens you send to the model and the number of tokens it generates. Sending a 10,000-word document to an LLM is far more expensive than sending a focused, 200-word chunk that contains the exact answer. Smart chunking directly translates to lower operational costs for your application.\n",
    "\n",
    "-----\n",
    "\n",
    "## 3\\. How Text Splitters Work in LangChain\n",
    "\n",
    "LangChain provides a suite of tools called `TextSplitters` to handle this process for us. The core idea is simple but powerful.\n",
    "\n",
    "A text splitter takes a list of `Document` objects (what we got from our loaders) and returns a new list of `Document` objects, where each new document is a chunk of one of the originals.\n",
    "\n",
    "```python\n",
    "# Conceptual Flow\n",
    "List[Large_Document] -> TextSplitter -> List[Small_Chunk_Document]\n",
    "```\n",
    "\n",
    "### Core Parameters: `chunk_size` and `chunk_overlap`\n",
    "\n",
    "When you define a text splitter, you'll almost always configure two key parameters:\n",
    "\n",
    "1.  **`chunk_size`**: This defines the desired maximum size of your chunks. This is typically measured in the number of characters. The splitter will try to create chunks that are at most this many characters long. It's a soft limit, meaning the splitter will try its best to respect it without breaking up words or sentences unnaturally.\n",
    "\n",
    "2.  **`chunk_overlap`**: This is the number of characters that will be repeated between two consecutive chunks.\n",
    "\n",
    "**Why is overlap so important?** Imagine you have a hard cutoff between two chunks. It's possible that a key piece of information, a sentence that links two ideas, gets split right in the middle.\n",
    "\n",
    "  * **Chunk 1 ends with:** \"The system's primary weakness is its lack of error handling.\"\n",
    "  * **Chunk 2 begins with:** \"This can be solved by implementing a try-except block.\"\n",
    "\n",
    "If a user's query is about solving the system's weakness, we need both of those sentences. Without overlap, we might only retrieve one of the chunks, losing the critical connection.\n",
    "\n",
    "**Analogy:** Think about reading a book. When you flip to a new page, your brain subconsciously remembers the last sentence or two from the previous page to maintain the flow of the story. `chunk_overlap` does exactly that for our RAG system.\n",
    "\n",
    "### Metadata Preservation\n",
    "\n",
    "This is a crucial feature. When a text splitter creates chunks from a source `Document`, it automatically **copies the metadata from the original document to each new chunk**.\n",
    "\n",
    "If our original document was `my_book.pdf`, then every chunk created from it will have metadata like `{'source': 'my_book.pdf'}`. This is essential for traceability. When our RAG system provides an answer, we can cite the exact source document the information came from.\n",
    "\n",
    "-----\n",
    "\n",
    "## 4\\. The Workhorse: `RecursiveCharacterTextSplitter`\n",
    "\n",
    "LangChain has several types of text splitters, but the one you will use 95% of the time is the `RecursiveCharacterTextSplitter`. It is the recommended and most versatile splitter for general-purpose text.\n",
    "\n",
    "**So, how does it work?**\n",
    "\n",
    "The \"recursive\" part of its name is the key. It maintains a list of separators. By default, this list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`.\n",
    "\n",
    "1.  It first tries to split the entire text by double newlines (`\\n\\n`), which usually correspond to paragraphs.\n",
    "2.  If any of the resulting chunks are still too large (i.e., bigger than `chunk_size`), it then takes that oversized chunk and tries to split it by the *next* separator: a single newline (`\\n`).\n",
    "3.  It continues this process recursively. If the chunks are still too big after splitting by newlines, it will try splitting by spaces, and finally, if all else fails, by individual characters.\n",
    "\n",
    "This hierarchical approach is brilliant because it tries to keep semantically related pieces of text together as long as possible. Paragraphs are kept together before being split into sentences, and sentences are kept together before being split into words. This helps maintain the meaning within your chunks.\n",
    "\n",
    "Let's see it in action.\n",
    "\n",
    "```python\n",
    "# First, let's make sure we have langchain installed\n",
    "# pip install langchain\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# This is our example text, a long string.\n",
    "# In a real application, this would come from a Document Loader.\n",
    "with open(\"sample_text.txt\", \"r\") as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "print(\"--- Original Text ---\")\n",
    "print(f\"Length of text: {len(sample_text)} characters\")\n",
    "print(sample_text[:500]) # Print the first 500 characters\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Instantiate our text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len, # Function to measure chunk size. len is the default.\n",
    "    is_separator_regex=False, # We are not using regular expressions for separators.\n",
    ")\n",
    "\n",
    "# Create the chunks. We'll wrap our text in a Document object first.\n",
    "# The splitter can work on raw strings, but it's best practice to work with Documents.\n",
    "doc = Document(page_content=sample_text, metadata={\"source\": \"sample_text.txt\", \"author\": \"AI Instructor\"})\n",
    "chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "print(f\"--- After Splitting ---\")\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Let's inspect the chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(f\"Content: '{chunk.page_content}'\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"Metadata: {chunk.metadata}\") # Notice the metadata is preserved!\n",
    "    print(\"-\" * 10)\n",
    "\n",
    "# Let's see the overlap in action\n",
    "print(\"\\n--- Demonstrating Overlap ---\")\n",
    "print(\"Last 70 characters of Chunk 1:\")\n",
    "print(f\"'{chunks[0].page_content[-70:]}'\")\n",
    "print(\"\\nFirst 70 characters of Chunk 2:\")\n",
    "print(f\"'{chunks[1].page_content[:70:]}'\")\n",
    "\n",
    "```\n",
    "\n",
    "Running this code with a sample text file will clearly show you how the original document is broken down. You'll see that each chunk is under the `chunk_size` limit and that the metadata is perfectly preserved. Most importantly, by printing the end of one chunk and the beginning of the next, you will visually confirm the `chunk_overlap`.\n",
    "\n",
    "### Other Splitters (For Awareness)\n",
    "\n",
    "While `RecursiveCharacterTextSplitter` is your go-to, you should be aware that others exist for specialized use cases:\n",
    "\n",
    "  * **`CharacterTextSplitter`**: A much simpler splitter that just splits on a single character (e.g., `\\n`) and doesn't have the recursive logic. It's less sophisticated.\n",
    "  * **Specialized Splitters**: LangChain has splitters designed for specific content types, like `MarkdownTextSplitter` and various `CodeTextSplitter`s for different programming languages (Python, Ruby, etc.). These understand the syntax of their respective formats to create more intelligent chunks.\n",
    "\n",
    "For now, master the `RecursiveCharacterTextSplitter`. It will serve you well.\n",
    "\n",
    "-----\n",
    "\n",
    "## 5\\. Best Practices & Troubleshooting\n",
    "\n",
    "Chunking is more of an art than an exact science. The optimal strategy depends heavily on your specific documents and your application's goal. Here are some tips.\n",
    "\n",
    "  * **Experiment with `chunk_size` and `chunk_overlap`**: There is no magic number. A good starting point is often a `chunk_size` of 500-1000 and a `chunk_overlap` of 50-100. Create your chunks, print them out, and read them. Do they make sense? Do they feel complete? Adjust the numbers and repeat.\n",
    "  * **Think About Your Data's Structure**: If your document is highly structured with clear headings and paragraphs, the recursive splitter will work beautifully. If you're dealing with messy, unstructured text (like a raw transcript), you might need to do some pre-processing first.\n",
    "  * **Beware of Losing Context**: If you find your RAG system is failing to answer questions that span across two different ideas, it might be because your chunks are too small or your overlap is insufficient. The answer is literally split in two, and your system is only finding one half.\n",
    "  * **Debugging is Your Friend**: `print()` is the most valuable tool here. When in doubt, `print(len(chunks))` and `print(chunks[0].page_content)`. Look at what the splitter is producing. Is it what you expected? If not, tweak your parameters.\n",
    "\n",
    "-----\n",
    "\n",
    "## 6\\. Our Updated RAG \"Archives\" Workflow\n",
    "\n",
    "Let's update our mental map of the RAG process. We've just added a critical pre-processing step.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Raw Data Sources <br/> (PDF, Web, TXT)] --> B{Document Loader};\n",
    "    B --> C[LangChain Documents <br/> (Large, single objects)];\n",
    "    C --> D{<b>Text Splitter (Chunking)</b>};\n",
    "    D --> E[LangChain Documents <br/> (Small, manageable chunks)];\n",
    "    E -- (This is our path for unstructured data) --> F[\"Next Stop: Embeddings & Vector Stores\"];\n",
    "```\n",
    "\n",
    "As you can see, **Chunking** sits right after loading and before the next major stage, which will involve making these chunks searchable. It's the essential preparation step that makes effective retrieval possible.\n",
    "\n",
    "-----\n",
    "\n",
    "## 7\\. Looking Ahead: From Chunks to Searchable Knowledge\n",
    "\n",
    "We've done a fantastic job. We've taken our massive knowledge base and broken it down into bite-sized, meaningful, and context-aware chunks. Each chunk is a neat little package of information, complete with metadata telling us where it came from.\n",
    "\n",
    "But this raises the next big question: We have hundreds, maybe thousands, of these chunks. How do we find the right one(s) when a user asks a question? We can't just scroll through them manually. We need a way to search them not just by keywords, but by *semantic meaning*.\n",
    "\n",
    "How do we turn our text chunks into something a computer can understand and compare for similarity?\n",
    "\n",
    "This is where the magic really begins. In our next section, we will dive into **Vector Embeddings**. We'll learn how to transform our text chunks into numerical vectors—a universal language for machine learning—that capture their underlying meaning. This transformation is the key that unlocks powerful, semantic search and the true heart of Retrieval-Augmented Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983bfef5-0ce1-47ad-8ce1-cf899fe43728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
