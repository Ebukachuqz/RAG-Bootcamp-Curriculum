{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdaa839-9d38-4b99-b6b4-83ddd8a8792f",
   "metadata": {},
   "source": [
    "# **Level 4: The Quest: Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd3c298-a2ed-41da-a2ad-b334c6b9256c",
   "metadata": {},
   "source": [
    "## Part 5: Hybrid Search – The Best of Both Worlds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678ed91-897a-4c22-a049-d6e3af83ec42",
   "metadata": {},
   "source": [
    "Hello everyone, and welcome back\\! We've come a long way on our retrieval journey. We've mastered individual techniques—keyword search, sparse search with BM25, and dense semantic search. Each one is powerful, but as we've seen, each has its own blind spots.\n",
    "\n",
    "So, the natural question is: can we get the best of all worlds? Can we build a system that understands the *meaning* behind our questions but never misses a crucial *keyword*? The answer is a resounding **yes**, and the technique is called **Hybrid Search**. Today, we'll see why this is the go-to method for building robust, production-grade RAG systems.\n",
    "\n",
    "-----\n",
    "\n",
    "## 1\\. Recap & The Motivation for Hybrid Search\n",
    "\n",
    "Before we build, let's remember the tools we have and why we need something more.\n",
    "\n",
    "### Recall: Individual Strengths & Weaknesses\n",
    "\n",
    "Let's quickly recap what we know about our existing search methods:\n",
    "\n",
    "  * **Keyword/Sparse Search (BM25):** This is our literalist. It's fantastic at finding exact matches and specific terms. If your query contains \"WGT-7890\" or \"Q2 2024 Financial Report,\" BM25 will find documents containing those exact strings with high precision. Its weakness? It has no real understanding of language. A search for \"how to fix my computer\" won't match a document that says \"troubleshooting a PC,\" even though they mean the same thing.\n",
    "\n",
    "  * **Dense Search (Vector Search):** This is our conceptualist. It's brilliant at understanding semantic meaning, synonyms, and paraphrasing. It knows that \"agile fox\" is very similar to \"quick vulpine creature.\" It excels at broad, conceptual queries. Its weakness? It can sometimes miss critical keywords if they aren't semantically important in the context of the whole document chunk. The product code \"WGT-7890\" might get lost in a sea of other, more \"meaningful\" words in the chunk's vector representation.\n",
    "\n",
    "### The Problem: Why Choose When You Can Have Both?\n",
    "\n",
    "This leads us to a critical problem. What if a user's query needs *both* keyword precision and semantic understanding?\n",
    "\n",
    "Imagine a user in a corporate environment searching: `\"What was the environmental impact of product WGT-7890 mentioned in the Q2 2024 report?\"`\n",
    "\n",
    "This single query has two distinct needs:\n",
    "\n",
    "1.  **Keyword Need:** It absolutely *must* find documents containing the specific identifier `\"WGT-7890\"` and the phrase `\"Q2 2024\"`. BM25 is perfect for this.\n",
    "2.  **Semantic Need:** It also needs to find information related to the *concept* of `\"environmental impact.\"` The relevant document might use phrases like \"carbon footprint,\" \"sustainability efforts,\" or \"eco-friendly manufacturing,\" which a dense search is perfectly suited to find.\n",
    "\n",
    "If we rely only on dense search, we risk missing the document with the specific product code. If we rely only on sparse search, we risk missing the documents that discuss sustainability without using the exact words \"environmental impact.\"\n",
    "\n",
    "### Introduction to Hybrid Search\n",
    "\n",
    "This is precisely where **Hybrid Search** comes in. It's not a new *type* of search, but a technique that **combines multiple search methods**—typically sparse (BM25) and dense (vector search)—to leverage their individual strengths and mitigate their weaknesses.\n",
    "\n",
    "> **Analogy:** Think of it like having two expert librarians working together on your request. One is a master archivist who's a wizard at finding exact book titles and serial numbers (our BM25 retriever). The other is a subject matter expert who deeply understands the *concept* of what you're looking for, even if you don't know the exact terms (our vector retriever). Hybrid search lets them collaborate, compare notes, and present you with a single, unified list of the most relevant resources.\n",
    "\n",
    "-----\n",
    "\n",
    "## 2\\. What is Hybrid Search? (The Synergistic Approach)\n",
    "\n",
    "At its heart, the concept is straightforward:\n",
    "\n",
    "1.  Run a query through multiple retrievers in parallel (e.g., one BM25 retriever and one vector store retriever).\n",
    "2.  Get back a ranked list of documents from each retriever.\n",
    "3.  Intelligently merge those separate lists into a single, re-ranked, and more relevant final list.\n",
    "\n",
    "The magic is in that third step: the \"combination\" or \"fusion\" of results. Simply tacking one list onto the end of another isn't good enough. A document that is highly relevant to *both* the keyword and semantic parts of the query should be boosted to the top.\n",
    "\n",
    "### The Combination Strategy: Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "The most common and effective algorithm for this is **Reciprocal Rank Fusion (RRF)**. It's a clever, score-agnostic way to combine ranked lists.\n",
    "\n",
    "**Simplified Explanation of RRF:**\n",
    "RRF looks at the rank (position) of each document in the various lists. It gives a higher final score to documents that appear in multiple lists, especially those that rank highly in them. A document that is \\#1 in the BM25 results and \\#3 in the vector search results will get a much higher combined score than a document that is \\#1 in one list but doesn't appear in the other at all.\n",
    "\n",
    "**Mathematical Intuition (Simplified):**\n",
    "You don't need to memorize the formula, but understanding the intuition is key. For each unique document retrieved, RRF calculates a final score by summing its reciprocal rank score from each list it appears in. The score for a single list is:\n",
    "\n",
    "$$\\text{RRF\\_Score} = \\sum_{i=1}^{N} \\frac{1}{k + \\text{rank}_i}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "  * `rank_i` is the document's rank (position) in retriever `i`'s results.\n",
    "  * `k` is a constant (LangChain uses a default of 60). This constant helps ensure that documents ranked lower down don't have their scores drop off too dramatically.\n",
    "\n",
    "> **Why is this so effective?** Let's consider a document's score. If it's rank 1, its score contribution is `1/(60+1)`. If it's rank 2, it's `1/(60+2)`. The key is that if a document appears in *both* lists, its final score is the *sum* of these individual scores, giving it a significant boost.\n",
    "\n",
    "**Why RRF?**\n",
    "A major advantage of RRF is that it doesn't need to look at the raw scores from the retrievers (like BM25 scores or cosine similarity scores). This is great because these scores are on different scales and notoriously difficult to compare or normalize meaningfully. RRF neatly sidesteps this problem by only caring about the *rank*.\n",
    "\n",
    "> **Analogy Revisited:** Imagine our two librarians each give you a ranked list of 5 books. If the same book is \\#1 on the archivist's list and \\#2 on the subject expert's list, RRF says, \"Wow, this must be incredibly relevant\\!\" and moves it to the very top of your final, combined list. A book that is \\#1 on one list but not on the other is still highly valued, but not as much as the one with dual validation.\n",
    "\n",
    "-----\n",
    "\n",
    "## 3\\. When to Use Hybrid Search\n",
    "\n",
    "This part is easy. For most production-grade RAG applications, hybrid search should be your default choice. The benefits almost always outweigh the slight increase in complexity.\n",
    "\n",
    "  * **Maximizing Recall and Precision:** It's the best way to ensure you are retrieving the most relevant documents possible, catching both keyword and semantic signals.\n",
    "  * **Handling Diverse Query Types:** Your RAG system will be robust enough to handle queries that are very specific (\"get me form 10-K\") and very broad (\"tell me about our company's strategy\").\n",
    "  * **Reducing Hallucinations:** The number one cause of LLM hallucination in RAG is poor context. By providing a richer, more accurate, and more comprehensive set of context documents, you drastically reduce the chances of the LLM inventing information.\n",
    "  * **Robustness:** Your retrieval system is no longer susceptible to the failure of a single search method. If dense search fails to find a keyword, sparse search is there as a backup, and vice-versa.\n",
    "  * **Real-world Applications:** This is a cornerstone of modern systems for enterprise search, customer support bots, legal e-discovery, and AI research assistants.\n",
    "\n",
    "-----\n",
    "\n",
    "## 4\\. Implementing Hybrid Search in LangChain: The `EnsembleRetriever`\n",
    "\n",
    "LangChain makes implementing hybrid search incredibly straightforward with the `EnsembleRetriever`. As the name suggests, it creates an \"ensemble\" of multiple retrievers that work together.\n",
    "\n",
    "You must have your individual retrievers configured and working first. From our previous lessons, this means you need an initialized `BM25Retriever` and a `VectorStoreRetriever` (like one from ChromaDB).\n",
    "\n",
    "The `EnsembleRetriever` has two main parameters:\n",
    "\n",
    "  * `retrievers`: A Python list containing the retriever instances you want to combine. For us, this will be `[bm25_retriever, vector_retriever]`.\n",
    "  * `weights`: (Optional) A list of floating-point numbers that must sum to 1.0. These weights determine the relative importance of each retriever. For example, `weights=[0.3, 0.7]` would mean that the vector retriever's rankings are given more influence in the final RRF calculation. The default is an equal weighting (e.g., `[0.5, 0.5]` for two retrievers), which is an excellent starting point.\n",
    "  * `c`: (Optional) This corresponds to the `k` constant in the RRF formula. The default of 60 is generally a good choice, so you rarely need to change it.\n",
    "\n",
    "Let's see it in action.\n",
    "\n",
    "-----\n",
    "\n",
    "## 5\\. Illustrative Code Examples: Building a Hybrid Retriever\n",
    "\n",
    "Time to get our hands dirty. We'll set up our environment, create our base retrievers, and then combine them.\n",
    "\n",
    "### Setup\n",
    "\n",
    "First, let's set up everything we need. This code assumes you've done the `pip install` for the required libraries and have your OpenAI API key set up if you're using it.\n",
    "\n",
    "```python\n",
    "# Ensure these are installed:\n",
    "# pip install langchain-community langchain-openai chromadb rank_bm25 sentence-transformers\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings # Or HuggingFaceEmbeddings for local\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- 0. Set up environment and mock data ---\n",
    "# For OpenAI:\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "# We'll use OpenAI embeddings for this lecture's conciseness, but you could\n",
    "# easily swap this for a local model.\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create some dummy documents to highlight the benefits of hybrid search.\n",
    "# Notice the mix of keyword-heavy and concept-heavy text.\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A fast, agile fox leaps over a lethargic canine.\",\n",
    "    \"The product code for the new widget is WGT-7890. It has an amazing feature set.\",\n",
    "    \"Our latest financial report for Q2 2024 shows significant growth in the technology sector.\",\n",
    "    \"Financial results for the second quarter of 2024 indicate robust performance.\",\n",
    "    \"The concept of quantum entanglement is fascinating, where particles become linked.\",\n",
    "    \"Resetting your password is easy. Navigate to the account settings and click 'Forgot Password'.\",\n",
    "    \"How do I change my PIN? You can modify your Personal Identification Number in the security section of your profile.\"\n",
    "]\n",
    "documents = [Document(page_content=t, metadata={\"source\": f\"doc_{i}\"}) for i, t in enumerate(texts)]\n",
    "\n",
    "# --- 1. Prepare chunks (from Document Loading and Chunking) ---\n",
    "# In a real app, this would come after loading documents. For this example, our texts are already small.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Total chunks prepared: {len(chunks)}\")\n",
    "\n",
    "# --- 2. Create a Vector Store (ChromaDB) for Dense Search ---\n",
    "# Using in-memory Chroma for simplicity. In a real project, you'd persist this.\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings_model)\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(\"\\nDense (VectorStore) Retriever created.\")\n",
    "\n",
    "# --- 3. Create a BM25 Retriever for Sparse Search ---\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = 5\n",
    "print(\"Sparse (BM25) Retriever created.\")\n",
    "```\n",
    "\n",
    "Now that we have our two base retrievers, `vector_retriever` and `bm25_retriever`, let's combine them.\n",
    "\n",
    "### Example 1: Basic Hybrid Search with Equal Weights\n",
    "\n",
    "This is the most common starting point. We give equal importance to both sparse and dense search.\n",
    "\n",
    "```python\n",
    "print(\"\\n--- Example 1: Basic Hybrid Search (Equal Weights) ---\")\n",
    "# Initialize the EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.5, 0.5] # 50/50 weighting\n",
    ")\n",
    "\n",
    "# A query with a strong keyword but also a conceptual part\n",
    "query_hybrid_1 = \"info on product WGT-7890\"\n",
    "print(f\"Query: '{query_hybrid_1}'\")\n",
    "retrieved_docs_hybrid_1 = ensemble_retriever.invoke(query_hybrid_1)\n",
    "\n",
    "print(f\"\\nRetrieved {len(retrieved_docs_hybrid_1)} documents with Hybrid Search:\")\n",
    "for i, doc in enumerate(retrieved_docs_hybrid_1):\n",
    "    print(f\"  {i+1}. Source: {doc.metadata.get('source', 'N/A')}, Content: '{doc.page_content}'\")\n",
    "\n",
    "# Now, let's compare this to what each retriever would have found on its own.\n",
    "# This is crucial for understanding WHY hybrid is better.\n",
    "print(\"\\n--- Individual Retriever Comparison for the Same Query ---\")\n",
    "print(\"BM25 Results (strong on keywords):\")\n",
    "bm25_results_1 = bm25_retriever.invoke(query_hybrid_1)\n",
    "for i, doc in enumerate(bm25_results_1[:3]): # Show top 3\n",
    "    print(f\"  {i+1}. Source: {doc.metadata.get('source', 'N/A')}, Content: '{doc.page_content}'\")\n",
    "\n",
    "print(\"\\nVectorStore Results (might miss the specific code):\")\n",
    "vector_results_1 = vector_retriever.invoke(query_hybrid_1)\n",
    "for i, doc in enumerate(vector_results_1[:3]): # Show top 3\n",
    "    print(f\"  {i+1}. Source: {doc.metadata.get('source', 'N/A')}, Content: '{doc.page_content}'\")\n",
    "```\n",
    "\n",
    "**Discussion:**\n",
    "Notice how BM25 immediately hones in on the document with `\"WGT-7890\"`. The vector search might also find it, but it might also pull in other documents about \"products\" or \"features.\" The `EnsembleRetriever` sees that the document containing `\"WGT-7890\"` is ranked highly by BM25 (and likely reasonably high by the vector retriever too), so RRF fuses these rankings and promotes it to the top, giving you the best, most precise result.\n",
    "\n",
    "### Example 2: Hybrid Search with Weighted Preference\n",
    "\n",
    "What if we know our users primarily ask conceptual questions, but we still want keyword search as a safety net? We can adjust the weights.\n",
    "\n",
    "```python\n",
    "print(\"\\n--- Example 2: Hybrid Search (Preferring Dense Search) ---\")\n",
    "# Let's say we generally prefer semantic results, but still want keyword backup.\n",
    "ensemble_retriever_biased = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.3, 0.7] # 30% BM25, 70% VectorStore\n",
    ")\n",
    "\n",
    "# A query that is strongly semantic\n",
    "query_hybrid_2 = \"how can I change my personal identification number?\"\n",
    "print(f\"Query: '{query_hybrid_2}'\")\n",
    "retrieved_docs_hybrid_2 = ensemble_retriever_biased.invoke(query_hybrid_2)\n",
    "\n",
    "print(f\"\\nRetrieved {len(retrieved_docs_hybrid_2)} documents with Biased Hybrid Search:\")\n",
    "for i, doc in enumerate(retrieved_docs_hybrid_2):\n",
    "    print(f\"  {i+1}. Source: {doc.metadata.get('source', 'N/A')}, Content: '{doc.page_content}'\")\n",
    "```\n",
    "\n",
    "**Discussion:**\n",
    "In this case, the query has strong semantic overlap with the document containing \"How do I change my PIN?\" and \"modify your Personal Identification Number\". The vector search will rank this document very highly. BM25 might not, as the exact keywords don't all appear. By giving the vector retriever a `0.7` weight, we are telling the RRF algorithm to put more emphasis on its ranking. The result is a list that prioritizes semantic relevance while still considering keyword matches.\n",
    "\n",
    "### Example 3: `MMR` within Hybrid Search\n",
    "\n",
    "Can we combine hybrid search with other techniques we've learned, like Maximal Marginal Relevance (MMR)? Yes\\! Remember, MMR is a feature of the *dense retriever* used to increase diversity in its results. We can configure our vector retriever to use MMR, and then pass *that* retriever into our `EnsembleRetriever`.\n",
    "\n",
    "MMR will first create a diverse list of documents from the dense search component. Then, that diverse list will be fed into the RRF algorithm to be combined with the BM25 results.\n",
    "\n",
    "```python\n",
    "print(\"\\n--- Example 3: Hybrid Search with MMR for the Dense Component ---\")\n",
    "# 1. First, create a vector retriever that uses MMR.\n",
    "mmr_vector_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 20} # Fetch more docs to find diverse ones\n",
    ")\n",
    "\n",
    "# 2. Now, create an ensemble using this new MMR-enabled retriever.\n",
    "# Note: BM25 doesn't have an MMR equivalent in LangChain's implementation.\n",
    "ensemble_retriever_mmr = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, mmr_vector_retriever], # Using the MMR vector retriever\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# A query that could have multiple relevant but different documents\n",
    "query_hybrid_3 = \"tell me about finances and security\"\n",
    "print(f\"Query: '{query_hybrid_3}'\")\n",
    "retrieved_docs_hybrid_3 = ensemble_retriever_mmr.invoke(query_hybrid_3)\n",
    "\n",
    "print(f\"\\nRetrieved {len(retrieved_docs_hybrid_3)} documents with MMR-Hybrid Search:\")\n",
    "for i, doc in enumerate(retrieved_docs_hybrid_3):\n",
    "    print(f\"  {i+1}. Source: {doc.metadata.get('source', 'N/A')}, Content: '{doc.page_content}'\")\n",
    "```\n",
    "\n",
    "**Discussion:**\n",
    "For a query like \"finances and security,\" MMR on the dense side will try to find documents that cover \"finances\" but are different from each other, and documents that cover \"security\" but are also different from each other. The BM25 retriever will find documents with the literal words \"financial\" and \"security.\" The RRF algorithm then combines these, giving you a final list that is not only relevant to the keywords and concepts but also diverse in its content.\n",
    "\n",
    "-----\n",
    "\n",
    "## 6\\. Benefits & Limitations of Hybrid Search\n",
    "\n",
    "Let's summarize the pros and cons.\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "  * **Enhanced Relevance:** The core benefit. Captures both lexical (keyword) and semantic (meaning) matches.\n",
    "  * **Improved Recall:** You are far more likely to retrieve *all* potentially relevant documents, regardless of how the user phrases their query.\n",
    "  * **Robustness:** Your system gracefully handles a much wider variety of query types and document content.\n",
    "  * **Reduced Hallucinations:** Better context in = better answers out. This is a direct and powerful way to improve the reliability of your LLM's responses.\n",
    "  * **Better User Experience:** Users get what they want without needing to be search experts.\n",
    "\n",
    "### Limitations & Considerations\n",
    "\n",
    "  * **Increased Complexity:** You are now managing two retrieval pipelines instead of one. It's more to set up, tune, and maintain.\n",
    "  * **Computational Cost:** You are running two searches instead of one. This adds latency. For most applications, the dramatic increase in relevance is worth the extra 50-100 milliseconds, but it's a trade-off to be aware of.\n",
    "  * **Weight Tuning:** While `[0.5, 0.5]` is a great default, finding the perfect `weights` for your specific use case might require some experimentation and evaluation.\n",
    "  * **Overlapping Information:** If you set `k` (the number of documents to retrieve) high for both underlying retrievers, you might get redundant information. RRF helps re-rank, but it doesn't eliminate the underlying overlap. This is a perfect segue into our next topic: **Reranking**, where a third model comes in to specifically address this issue.\n",
    "\n",
    "-----\n",
    "\n",
    "## 7\\. Integrating Hybrid Retriever into a RAG Chain\n",
    "\n",
    "The beauty of LangChain's design is its modularity. The `EnsembleRetriever` conforms to the same `Retriever` interface as all our other retrievers. This means plugging it into a full RAG chain is as simple as swapping one variable.\n",
    "\n",
    "Let's take the RAG chain we've used before and plug our `ensemble_retriever` right in.\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# We will use the equally-weighted ensemble_retriever from Example 1\n",
    "retriever = ensemble_retriever\n",
    "\n",
    "# Set up the LLM and prompt template from previous lessons\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "rag_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI assistant. Use the following retrieved context to answer the user's question. If the information is not in the context, state that you cannot answer based on the provided information.\\n\\nContext:\\n{context}\"),\n",
    "    (\"user\", \"Question: {question}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"A simple helper function to format the retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# The RAG chain structure remains the same!\n",
    "rag_chain_hybrid = (\n",
    "    {\"context\": retriever | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"\\n--- Running a Full RAG Chain with our Hybrid Retriever ---\")\n",
    "\n",
    "# This query requires finding a specific product code (keyword) AND a concept (quantum entanglement)\n",
    "query_rag = \"What is the product code for the new widget and what is quantum entanglement?\"\n",
    "print(f\"RAG Query: '{query_rag}'\")\n",
    "\n",
    "# Invoking the chain\n",
    "response_rag = rag_chain_hybrid.invoke(query_rag)\n",
    "print(\"\\n--- RAG Response ---\")\n",
    "print(response_rag)\n",
    "\n",
    "# Another example that relies on semantic understanding\n",
    "query_rag_2 = \"Where can I change my PIN?\"\n",
    "print(f\"\\n\\n--- Another RAG Query ---\")\n",
    "print(f\"RAG Query: '{query_rag_2}'\")\n",
    "\n",
    "response_rag_2 = rag_chain_hybrid.invoke(query_rag_2)\n",
    "print(\"\\n--- RAG Response ---\")\n",
    "print(response_rag_2)\n",
    "```\n",
    "\n",
    "**Discussion:**\n",
    "This is the payoff. Because our `ensemble_retriever` is so effective, it can retrieve context for both parts of the first query—the document with \"WGT-7890\" and the document about \"quantum entanglement.\" This gives the LLM all the necessary information to construct a complete and accurate answer. The chain's structure didn't need to change at all. We just got smarter about the `retriever` we used.\n",
    "\n",
    "-----\n",
    "\n",
    "## 8\\. Key Takeaways\n",
    "\n",
    "> **Hybrid Search: The Production Standard**\n",
    ">\n",
    ">   * **What It Is:** A technique that combines multiple search methods (usually sparse/keyword and dense/semantic) to get the best of both worlds.\n",
    ">   * **How It Works:** It runs queries against multiple retrievers in parallel and then uses an algorithm like **Reciprocal Rank Fusion (RRF)** to intelligently merge the ranked lists.\n",
    ">   * **Why RRF?** RRF is effective because it's score-agnostic (only uses rank) and boosts documents that are validated by multiple retrievers.\n",
    ">   * **LangChain Implementation:** Use the `EnsembleRetriever`, passing it a list of your configured `retrievers` and optional `weights`.\n",
    ">   * **The Main Benefit:** A massive improvement in relevance, recall, and robustness, leading to better RAG outputs and fewer hallucinations. It should be your default choice for serious applications.\n",
    ">   * **Integration:** It's a drop-in replacement for any other retriever in your RAG chain, thanks to LangChain's unified `Retriever` interface.\n",
    "\n",
    "-----\n",
    "\n",
    "## 9\\. Exercises & Thought Experiments\n",
    "\n",
    "1.  **Build Your Own Hybrid Chain:**\n",
    "\n",
    "      * Take a small dataset you've used before (or create a new one with 5-10 text files).\n",
    "      * Follow the steps in this lecture to create a `BM25Retriever` and a `VectorStoreRetriever` (using ChromaDB).\n",
    "      * Combine them into an `EnsembleRetriever`.\n",
    "      * Craft three queries to test it:\n",
    "        1.  A query that is purely keyword-based (e.g., contains a unique name or code).\n",
    "        2.  A query that is purely semantic (e.g., asks a question using different words than the source text).\n",
    "        3.  A query that is a mix of both.\n",
    "      * For each query, run it through the `EnsembleRetriever`, the `BM25Retriever` alone, and the `VectorStoreRetriever` alone. Compare the results and write down your observations.\n",
    "\n",
    "2.  **Experiment with Weights:**\n",
    "\n",
    "      * Using your setup from Exercise 1 and your \"mixed\" query, adjust the `weights` in the `EnsembleRetriever`.\n",
    "      * Try `weights=[0.1, 0.9]` (heavily favoring dense search).\n",
    "      * Try `weights=[0.9, 0.1]` (heavily favoring sparse search).\n",
    "      * How do the final ranked lists change? Discuss a scenario where each of these biased weightings might be preferable to the default `[0.5, 0.5]`.\n",
    "\n",
    "3.  **RRF vs. Simple Concatenation (Conceptual):**\n",
    "\n",
    "      * Imagine you didn't have RRF. A simpler approach might be to take the top 3 results from BM25 and the top 3 results from the vector search, remove duplicates, and just concatenate them into a list of (up to) 6 documents.\n",
    "      * How would this \"simple concatenation\" approach differ from the results you get with RRF?\n",
    "      * Why is RRF's method of re-ranking based on shared rank fundamentally better for producing the most relevant final list? What information is lost in the simple approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35aa14-937d-4686-b6bb-3a3bfbf8fb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
