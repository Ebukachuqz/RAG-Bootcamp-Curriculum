{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820cb7f6-13ad-459b-873e-74b5a203722f",
   "metadata": {},
   "source": [
    "# **Level 4 - The Quest: Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e961cf4-ba67-456e-ba07-9e4db0f9bc18",
   "metadata": {},
   "source": [
    "## Part 6: Reranking & Rerankers – The Final Polish for Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dfcb0e-ae2d-4216-afb3-c5214a8fc7d9",
   "metadata": {},
   "source": [
    "Hello everyone, and welcome back\\!\n",
    "\n",
    "In our journey so far, we've built a powerful toolkit for the \"Retrieval\" part of RAG. We've mastered Document Loading, Chunking, Embeddings, and setting up Vector Stores. We've explored different search strategies—Dense Search for semantic meaning, Keyword/Sparse Search for exact matches, and Hybrid Search to get the best of both worlds.\n",
    "\n",
    "You now know how to cast a wide net and pull in a set of documents that are *potentially* relevant to a user's query. But that brings us to a critical question... what's next?\n",
    "\n",
    "-----\n",
    "\n",
    "## 1\\. Recap & The \"Good Enough\" Problem\n",
    "\n",
    "So far, our process looks something like this: a user asks a question, and our retriever (whether it's a `VectorStoreRetriever`, `BM25Retriever`, or an `EnsembleRetriever`) dives into our knowledge base and pulls out its best guess at the top 'K' relevant chunks.\n",
    "\n",
    "This is a fantastic start. But \"good\" isn't always \"perfect.\"\n",
    "\n",
    "Let's think about the problem we're facing. Our retriever might return 10 documents. The most relevant, game-changing piece of information might be in the document ranked \\#7. But the documents at positions \\#1, \\#2, and \\#3 are only \"okay.\" When we pass this ordered list to an LLM, its attention isn't infinite. It might get bogged down by the mediocre context at the beginning and miss the crucial detail buried deeper in the list.\n",
    "\n",
    "This is the \"Good Enough\" problem. Our initial retrieval is good enough to *find* the candidate documents, but it's not always precise enough to *rank them perfectly* for the LLM's immediate use.\n",
    "\n",
    "> **Analogy: The Chef's Ingredients**\n",
    ">\n",
    "> Imagine you're a chef preparing a Michelin-star tomato soup. You send your assistant (the retriever) to the pantry to grab some tomatoes. The assistant comes back with a basket of 10 tomatoes. They're all tomatoes, for sure. But some are perfectly ripe and vibrant, while others are a bit bruised or under-ripe.\n",
    ">\n",
    "> You wouldn't just dump the whole basket into the pot in the order they were picked. As the master chef, you would perform a second step: you'd quickly inspect all 10 tomatoes, select the 3-4 absolute best ones, and place them right on your cutting board, ready for cooking.\n",
    ">\n",
    "> **Reranking is this final, expert selection process for your RAG system.** It takes the retrieved \"ingredients\" and ensures only the highest quality ones are presented to the LLM.\n",
    "\n",
    "-----\n",
    "\n",
    "## 2\\. What is Reranking? (The Quality Control Layer)\n",
    "\n",
    "So, what exactly is this \"expert selection\" step?\n",
    "\n",
    "**Definition:** **Reranking** is a crucial post-retrieval step where an initial set of retrieved documents is re-scored and reordered to prioritize the most relevant information for a given query.\n",
    "\n",
    "The primary purpose of reranking is to dramatically improve the **precision** of the context we send to the LLM. We want to ensure that the very first documents the LLM sees are the absolute best, most relevant ones, reducing noise and boosting the quality of the final answer.\n",
    "\n",
    "This introduces a more sophisticated, two-stage retrieval process.\n",
    "\n",
    "### How it Works: Two-Stage Retrieval\n",
    "\n",
    "1.  **Stage 1 (Initial Retrieval):** We use a fast, high-recall retriever like the ones we've already built (dense, sparse, or hybrid). The goal here is to **cast a wide net** and gather a larger-than-needed number of candidate documents (e.g., retrieve 20-50 documents). This stage prioritizes *finding* all potential matches, even if the ordering isn't perfect.\n",
    "\n",
    "2.  **Stage 2 (Reranking):** A second, more computationally intensive but highly accurate model—the **reranker**—takes over. It individually evaluates each of the 20-50 candidate documents against the original query to calculate a much more precise relevance score. Based on these new scores, the documents are reordered, and we select a much smaller, highly relevant subset (e.g., the top 3-5) to finally pass to the LLM.\n",
    "\n",
    "> **Analogy: The Diligent Librarian**\n",
    ">\n",
    ">   * **Stage 1 (Retrieval):** You ask the librarian for books on \"the economic impact of the Renaissance in Florence.\" The librarian quickly runs to the history section and pulls 20 books that have \"Renaissance,\" \"Florence,\" or \"Economics\" in their titles or chapter headings. This is fast and gets a lot of candidates.\n",
    ">   * **Stage 2 (Reranking):** You (the reranker) don't have time to read all 20 books. Instead, you take the stack of 20, read the back cover, the table of contents, and the introduction of each one. Based on this deeper look, you select the 3 books that are *most precisely* about your specific topic and put them on top of your pile. You've just reranked the documents for maximum relevance.\n",
    "\n",
    "-----\n",
    "\n",
    "## 3\\. Why is Reranking So Important for RAG?\n",
    "\n",
    "You might be thinking, \"This sounds like an extra step. Is it really worth the effort?\" Absolutely. Here's the impact of adding a reranking stage:\n",
    "\n",
    "  * **Combats LLM Positional Bias:** This is a huge one. Research and empirical evidence show that LLMs don't weigh all parts of their context window equally. They tend to pay more attention to information at the **very beginning and very end** of the provided context. This is often called the \"lost in the middle\" problem. If your most relevant document is stuck in the middle of a list of 10, the LLM might gloss over it. Reranking ensures your best-shot context is placed right at the start, where the LLM is paying the most attention.\n",
    "\n",
    "  * **Improves Precision & Reduces Noise:** By filtering out less relevant documents, you provide a cleaner, more potent signal to the LLM. This directly translates to more focused, accurate, and relevant answers because the model isn't distracted by irrelevant information.\n",
    "\n",
    "  * **Reduces Hallucinations:** When the context is highly relevant and contains the answer, the LLM is far less likely to \"make things up\" or hallucinate. Providing high-precision context is one of the best ways to ground the model in facts.\n",
    "\n",
    "  * **Cost-Efficiency (Indirectly):** While reranking adds a small computational cost, it can save you money on LLM calls. If you can confidently reduce the number of documents you send to the LLM from, say, 10 to 4, you are reducing the number of tokens in your prompt. Over thousands or millions of calls, this can lead to significant savings.\n",
    "\n",
    "  * **Increases Robustness:** Your RAG system becomes more resilient. Even if your initial retrieval stage is imperfect, the reranking step acts as a powerful corrective measure, polishing the results before they reach the LLM.\n",
    "\n",
    "-----\n",
    "\n",
    "## 4\\. Introducing Reranker Models: Cross-Encoders\n",
    "\n",
    "To understand how rerankers work their magic, we need to introduce a new type of model architecture and contrast it with what we've used for embeddings.\n",
    "\n",
    "### Key Distinction: Bi-Encoders vs. Cross-Encoders\n",
    "\n",
    "  * **Bi-Encoders (Our Embedding Models):**\n",
    "\n",
    "      * This is the architecture used by the embedding models we've been working with (e.g., `all-MiniLM-L6-v2`, `BAAI/bge-small-en-v1.5`).\n",
    "      * \"Bi\" means two. A bi-encoder processes two items, the **query and the document, independently**.\n",
    "      * It creates one vector embedding for the query and separate vector embeddings for all the documents. The retrieval process then involves comparing the query vector to all the document vectors (e.g., using cosine similarity).\n",
    "      * **Advantage:** Extremely fast and scalable. You can pre-calculate all your document embeddings and store them. When a query comes in, you only need to embed the query once and then perform a fast vector search. This is why it's perfect for Stage 1 retrieval over millions of documents.\n",
    "\n",
    "  * **Cross-Encoders (Our Reranker Models):**\n",
    "\n",
    "      * A cross-encoder takes a different approach. It takes the **(query, document) pair as a single input** and processes them *together* through one transformer model.\n",
    "      * Because the model sees both the query and the document simultaneously, it can perform much deeper, token-by-token attention and comparison between them. It builds a far richer understanding of how relevant the document is to the specific query.\n",
    "      * **Output:** The output is not a vector. It's a single scalar value (a number, often between 0 and 1 or a raw logit) that represents the relevance score.\n",
    "      * **Trade-off:** This deep analysis is much more computationally expensive and therefore slower. You can't use a cross-encoder to search over millions of documents—it would take forever. But it's perfect for accurately re-scoring a small list of 20-50 candidates from our Stage 1 retriever.\n",
    "\n",
    "This trade-off is the entire reason the two-stage retrieval process exists: use the fast, scalable Bi-Encoder for initial retrieval, and the slow, accurate Cross-Encoder for final reranking.\n",
    "\n",
    "-----\n",
    "\n",
    "## 5\\. Implementing Reranking in LangChain with Open-Source Models\n",
    "\n",
    "Theory is great, but let's get practical. LangChain makes integrating rerankers incredibly straightforward with a dedicated component.\n",
    "\n",
    "### The `ContextualCompressionRetriever`\n",
    "\n",
    "This is the main workhorse for reranking in LangChain. It's a special type of retriever that wraps two key components:\n",
    "\n",
    "1.  `base_retriever`: This is your existing retriever—the one you built in the last section (e.g., a vector retriever, or your hybrid `EnsembleRetriever`). This retriever performs Stage 1.\n",
    "2.  `base_compressor`: This is where the reranker comes in. The \"compressor\" takes the documents from the `base_retriever` and performs the reranking and filtering logic. This is Stage 2.\n",
    "\n",
    "Let's see how to build this using a powerful, free, open-source reranker from the Sentence Transformers library.\n",
    "\n",
    "### Option 1: Using `SentenceTransformerReranker`\n",
    "\n",
    "This is one of the easiest and most popular ways to add high-quality reranking to your RAG pipeline.\n",
    "\n",
    "**Step 0: Installation**\n",
    "\n",
    "First, make sure you have the necessary library.\n",
    "\n",
    "```bash\n",
    "pip install sentence-transformers\n",
    "```\n",
    "\n",
    "**Step 1: Choose a Cross-Encoder Model**\n",
    "\n",
    "You can find many pre-trained cross-encoder models on the Hugging Face Hub. When searching, look for models with \"cross-encoder\" or \"rerank\" in their names. Good choices for general-purpose tasks include:\n",
    "\n",
    "  * `cross-encoder/ms-marco-MiniLM-L6-v2`: A very popular, well-balanced, and fast model.\n",
    "  * `BAAI/bge-reranker-base`: A powerful model from the Beijing Academy of Artificial Intelligence (BAAI).\n",
    "  * `BAAI/bge-reranker-large`: An even more powerful, but larger and slower, version from BAAI.\n",
    "\n",
    "Always check the model card on Hugging Face for its license and performance benchmarks to ensure it fits your project's needs.\n",
    "\n",
    "**Step 2: Let's Understand the Reranker's Core Logic (Manual Demo)**\n",
    "\n",
    "Before we plug it into LangChain, let's see what a cross-encoder does on its own. It's surprisingly simple.\n",
    "\n",
    "```python\n",
    "#\n",
    "# A quick standalone demonstration of how a cross-encoder works.\n",
    "#\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load a pre-trained cross-encoder model\n",
    "# This is a lightweight but effective model fine-tuned for semantic search.\n",
    "reranker_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "\n",
    "# Let's create some query-document pairs to score\n",
    "query = \"What is the capital of France?\"\n",
    "documents = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"The Eiffel Tower is a famous landmark in Paris.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"France is a country in Western Europe known for its cuisine and wine.\"\n",
    "]\n",
    "\n",
    "# The model expects a list of (query, document) pairs\n",
    "sentence_pairs = [(query, doc) for doc in documents]\n",
    "\n",
    "# Let's get the scores\n",
    "# The output is a list of scores, one for each pair. Higher score = more relevant.\n",
    "scores = reranker_model.predict(sentence_pairs)\n",
    "\n",
    "# Let's see the results\n",
    "print(\"--- Manual Cross-Encoder Scoring ---\")\n",
    "for i in range(len(scores)):\n",
    "    print(f\"Score: {scores[i]:.4f}\\t Document: {documents[i]}\")\n",
    "\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "--- Manual Cross-Encoder Scoring ---\n",
    "Score: 9.6373     Document: Paris is the capital of France.\n",
    "Score: 5.4880     Document: The Eiffel Tower is a famous landmark in Paris.\n",
    "Score: -5.3259    Document: Berlin is the capital of Germany.\n",
    "Score: 2.1084     Document: France is a country in Western Europe known for its cuisine and wine.\n",
    "```\n",
    "\n",
    "Notice how the model gives a very high score to the direct answer, a medium score to a semantically related document, a low score to a document about France in general, and a very low (negative) score to the irrelevant document about Berlin. This scoring precision is what we want to leverage.\n",
    "\n",
    "**Step 3: Putting It All Together in LangChain**\n",
    "\n",
    "Now, let's build the full pipeline. We'll set up a basic retriever and then wrap it with our reranker.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# For the reranker\n",
    "from langchain_community.document_compressors import SentenceTransformerReranker\n",
    "from langchain.retrievers.document_compressors import ContextualCompressionRetriever\n",
    "\n",
    "# For our demo RAG chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# --- 1. Setup Base Retriever (What we've learned before) ---\n",
    "\n",
    "# Assume we have some text data\n",
    "doc_text = \"\"\"\n",
    "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\n",
    "The Louvre, or the Louvre Museum, is the world's most-visited museum, and a historic landmark in Paris, France.\n",
    "The Arc de Triomphe is one of the most famous monuments in Paris, standing at the western end of the Champs-Élysées.\n",
    "Paris is the capital and most populous city of France. The city is known for its art, fashion, gastronomy and culture.\n",
    "Berlin, the capital of Germany, is known for its exceptional art scene and modern landmarks.\n",
    "The Brandenburg Gate is a famous landmark in Berlin.\n",
    "\"\"\"\n",
    "with open(\"paris_facts.txt\", \"w\") as f:\n",
    "    f.write(doc_text)\n",
    "\n",
    "\n",
    "# Load and split the document\n",
    "loader = TextLoader(\"paris_facts.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Set up embeddings and vector store\n",
    "# Make sure you have your OPENAI_API_KEY set in your environment\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# This is our Stage 1 retriever. It will fetch 10 documents.\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "\n",
    "# --- 2. Initialize the Reranker ---\n",
    "\n",
    "# Initialize LangChain's reranker compressor\n",
    "# It uses the specified cross-encoder model\n",
    "# top_n determines how many documents are returned after reranking\n",
    "compressor = SentenceTransformerReranker(\n",
    "    model_name=\"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "    top_n=3  # Return the top 3 most relevant documents\n",
    ")\n",
    "\n",
    "# --- 3. Create the Contextual Compression Retriever ---\n",
    "\n",
    "# This is our Stage 2 retriever.\n",
    "# It chains the base_retriever and the reranker.\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "# --- 4. Use the Reranked Retriever ---\n",
    "\n",
    "query = \"What are some famous landmarks in Paris?\"\n",
    "\n",
    "# Let's compare the results!\n",
    "\n",
    "print(\"--- 4a. Results from Base Retriever (NO Reranking) ---\")\n",
    "base_retrieved_docs = base_retriever.invoke(query)\n",
    "for i, doc in enumerate(base_retrieved_docs):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content}\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 4b. Results from Compression Retriever (WITH Reranking) ---\")\n",
    "reranked_docs = compression_retriever.invoke(query)\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    # The reranker adds a 'relevance_score' to the document metadata\n",
    "    score = doc.metadata.get('relevance_score', 'N/A')\n",
    "    print(f\"Document {i+1} (Score: {score:.4f}):\\n{doc.page_content}\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Using it in a full RAG Chain ---\n",
    "\n",
    "# Now you can use this `compression_retriever` just like any other retriever\n",
    "# in your RAG chains.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question based only on the following context:\\n\\n\"\n",
    "    \"{context}\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# The RAG chain now uses our powerful, reranked retriever\n",
    "reranked_rag_chain = (\n",
    "    {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"\\n--- 5. Final Answer from Reranked RAG Chain ---\")\n",
    "final_answer = reranked_rag_chain.invoke(query)\n",
    "print(final_answer)\n",
    "\n",
    "```\n",
    "\n",
    "When you run this, you will see a clear difference. The base retriever might pull up documents about Berlin because of the term \"landmarks.\" However, the reranked retriever will be much more precise, identifying the chunks about the Eiffel Tower, the Louvre, and the Arc de Triomphe and pushing them to the top with high relevance scores, while discarding the irrelevant Berlin documents entirely. The final answer from the RAG chain will be crisp and focused only on Paris.\n",
    "\n",
    "### Option 2: FlashRank (A Lighter, Faster Alternative)\n",
    "\n",
    "For scenarios where speed is absolutely critical and you're running on CPU, you might consider `FlashRank`. It's a very lightweight and optimized library for reranking. The integration pattern in LangChain is very similar.\n",
    "\n",
    "You would install it (`pip install flashrank`) and then use `FlashrankRerank` from `langchain_community.document_compressors`. It's a great tool to have in your back pocket.\n",
    "\n",
    "-----\n",
    "\n",
    "## 6\\. Paid/Proprietary Reranker Alternatives\n",
    "\n",
    "While open-source models are fantastic and sufficient for many use cases, the commercial world offers extremely high-performance rerankers that are worth knowing about, especially for enterprise-grade applications.\n",
    "\n",
    "  * **Cohere Rerank:** Widely considered a leader in this space. Cohere provides a rerank endpoint via an API that is highly optimized for performance and accuracy across many languages. It's a simple API call but requires an API key and incurs costs.\n",
    "  * **Voyage AI Rerank:** Another strong commercial player offering a high-performance reranking model via an API.\n",
    "\n",
    "The choice between open-source and proprietary often comes down to a balance of performance requirements, budget, data privacy concerns, and infrastructure control.\n",
    "\n",
    "-----\n",
    "\n",
    "## 7\\. Best Practices for Reranking\n",
    "\n",
    "To get the most out of reranking, keep these pro-tips in mind:\n",
    "\n",
    "  * **Generous Initial Retrieval (`k`):** Your reranker can only reorder what it's given. If the most relevant document isn't in the initial set retrieved by your `base_retriever`, the reranker can't magically find it. It's a common mistake to set the initial `k` too low. A good starting point is to retrieve **2x to 5x** the number of documents you ultimately want to pass to the LLM. For example, retrieve `k=25` to rerank and select the `top_n=5`.\n",
    "  * **Choose `top_n` Carefully:** This parameter in your reranker compressor (`SentenceTransformerReranker(top_n=...)`) determines how many documents are sent to the LLM.\n",
    "      * Too few (`top_n=1`): The LLM might lack sufficient context to form a comprehensive answer.\n",
    "      * Too many (`top_n=10`): You might start to dilute the signal with less relevant information, exceed the LLM's context window, or increase costs unnecessarily. A value between **3 and 5** is often a sweet spot.\n",
    "  * **Model Quality Matters:** The quality of your reranker is paramount. Use well-established, pre-trained cross-encoder models that have good benchmarks on tasks similar to yours (e.g., MS MARCO is a common dataset for information retrieval).\n",
    "  * **Mind the Latency:** Reranking is an extra step and adds latency to your RAG pipeline. This is a direct trade-off for accuracy. For real-time applications, you must test the latency and choose a model that meets your performance budget. Lighter models (`MiniLM`) are faster than larger ones (`bge-reranker-large`). Using a GPU for inference will also dramatically speed things up.\n",
    "  * **Preserve Your Metadata:** Ensure that any important metadata from your original documents is carried through the reranking process. LangChain's `ContextualCompressionRetriever` handles this well, but it's always good practice to check that the final documents still contain the source information you need.\n",
    "\n",
    "-----\n",
    "\n",
    "## 8\\. Troubleshooting Common Reranking Issues\n",
    "\n",
    "If you've implemented reranking but aren't seeing the improvements you expect, here are a few things to check:\n",
    "\n",
    "  * **Problem: \"My answers are still bad or irrelevant after reranking\\!\"**\n",
    "\n",
    "      * **Diagnosis:** The most likely culprit is your Stage 1 retrieval. The reranker is not a magician.\n",
    "      * **Solution:** Manually inspect the output of your `base_retriever.invoke(query)`. Is the correct information even present in that initial, wider set of documents? If not, no amount of reranking will help. You need to go back and improve your initial retrieval. This could mean:\n",
    "          * Improving your chunking strategy.\n",
    "          * Switching to a better embedding model (bi-encoder).\n",
    "          * Implementing a hybrid search retriever if you haven't already.\n",
    "\n",
    "  * **Problem: \"My RAG pipeline is too slow now\\!\"**\n",
    "\n",
    "      * **Diagnosis:** The cross-encoder is the bottleneck.\n",
    "      * **Solution:**\n",
    "          * Are you running on a CPU when a GPU is available? Move your model to a GPU for a significant speedup.\n",
    "          * Is your initial retrieval `k` excessively large? Retrieving 200 documents to rerank down to 3 will be slow. Try reducing the `k` in your `base_retriever`.\n",
    "          * Consider using a smaller, faster cross-encoder model like `cross-encoder/ms-marco-MiniLM-L6-v2` or `FlashRank` instead of a larger model like `BAAI/bge-reranker-large`.\n",
    "\n",
    "-----\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    ">   * **Reranking is a Two-Stage Process:** Use a fast retriever (Bi-Encoder) to get a wide set of candidate documents, then use a slower, more accurate model (Cross-Encoder) to re-score and select the absolute best ones.\n",
    ">   * **Cross-Encoders vs. Bi-Encoders:** Bi-Encoders create embeddings independently (fast, for retrieval). Cross-Encoders process (query, document) pairs together (slow, for accurate reranking).\n",
    ">   * **Primary Goal:** To improve **precision** and combat **LLM positional bias** by placing the most relevant context at the very top of the list for the LLM.\n",
    ">   * **LangChain Implementation:** Use the `ContextualCompressionRetriever`, which wraps a `base_retriever` (your Stage 1) and a `base_compressor` (your reranker, like `SentenceTransformerReranker`).\n",
    ">   * **Tune Your Knobs:** The key parameters are the initial retrieval count `k` (make it generous) and the final reranked count `top_n` (make it precise).\n",
    "\n",
    "-----\n",
    "\n",
    "## Exercises & Thought Experiments\n",
    "\n",
    "1.  **See the Impact:**\n",
    "\n",
    "      * Take a query for your own data. Run your `base_retriever` and print out the page content of the top 10 results. Note their order.\n",
    "      * Now, run the same query through your `compression_retriever` with `top_n=3`. Compare the three documents you get back with the original top 10. Did the order change? Did a document that was originally at position \\#5 or \\#6 jump to \\#1?\n",
    "\n",
    "2.  **The `top_n` Experiment:**\n",
    "\n",
    "      * Using your `compression_retriever`, run the same query three times, setting `top_n` to `2`, then `5`, then `10`.\n",
    "      * Observe the context that would be sent to the LLM in each case. Discuss with a partner: How might the LLM's answer differ in quality or completeness with each `top_n` value? When would you want a smaller `top_n` versus a larger one?\n",
    "\n",
    "3.  **Conceptual Model Comparison:**\n",
    "\n",
    "      * Imagine you are building a RAG system for a high-stakes legal document search, where accuracy is paramount and a few seconds of extra latency is acceptable. Would you lean towards `cross-encoder/ms-marco-MiniLM-L6-v2` or `BAAI/bge-reranker-large`? Why?\n",
    "      * Now imagine you are building a real-time customer service chatbot that needs to answer questions instantly. What would be your primary concern, and how might that influence your choice of reranker model or even the `k` and `top_n` parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcf0eb5-cbf2-4caa-8d6f-700c950dc281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
