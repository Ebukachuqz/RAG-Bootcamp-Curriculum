{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9c6f6b-5c66-4ea0-840f-db068d14bb25",
   "metadata": {},
   "source": [
    "# **Level 4: The Quest: Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014f8d6-7204-4628-ba90-09cff82cc5c0",
   "metadata": {},
   "source": [
    "## Part 7: Evaluation – Is Your Quest a Success?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4aaa1-dfd4-4cbd-85fa-009d202d2e56",
   "metadata": {},
   "source": [
    "Welcome to the grand finale of our deep dive into retrieval\\! This is where we bring everything together and ask the most critical question of all: how do we know if our quest for knowledge is actually a success?\n",
    "\n",
    "-----\n",
    "\n",
    "## Recap & Bridge: The Grand Finale of Retrieval\n",
    "\n",
    "So far on our journey, we've assembled an impressive toolkit. We started by understanding the fundamental goal of retrieval: to find the most relevant pieces of information from our knowledge base to help an LLM generate a great answer.\n",
    "\n",
    "We explored a variety of powerful questing strategies:\n",
    "\n",
    "  * **Keyword and Sparse Search:** The classic, reliable methods for finding exact word matches.\n",
    "  * **Dense Search:** The modern, semantic approach using embeddings to find documents that are conceptually similar, even if they don't share the same keywords.\n",
    "  * **Hybrid Search:** The \"best of both worlds\" approach, combining the strengths of sparse and dense methods to achieve robust performance.\n",
    "  * **Reranking:** The final polish, where we take the initial retrieved results and use a more sophisticated model to reorder them, pushing the most relevant documents to the very top.\n",
    "\n",
    "We've built some incredibly powerful retrieval systems. But a powerful tool is only useful if you know how to wield it effectively. That leads us to the central question for today's session: **\"We've built these systems, but how do we *know* if they're actually working well? How do we measure if our AI is truly 'questing' for the right knowledge effectively?\"**\n",
    "\n",
    "This is where **Evaluation** comes in. It's not just a final checkmark; it's the essential, ongoing process that transforms a good prototype into a reliable, trustworthy, and high-performing RAG application. Today, we learn how to measure our success.\n",
    "\n",
    "-----\n",
    "\n",
    "## Why Evaluate Retrieval? The Indispensable Step\n",
    "\n",
    "You might run a few queries, and the results might *look* pretty good. It's easy to fall into the trap of thinking, \"This feels right.\" But intuition is not enough. What seems good to us as developers, with our full knowledge of the system and data, might be confusing or insufficient for the LLM or a completely different experience for the end-user.\n",
    "\n",
    "This is why formal evaluation is an indispensable step in building any serious RAG system.\n",
    "\n",
    "### Benefits of Evaluation\n",
    "\n",
    "  * **Objective Measurement:** Evaluation allows us to move beyond subjective feelings and quantify the performance of our retriever with concrete numbers. We can definitively say, \"This new chunking strategy improved our performance by 15%,\" instead of \"I think this one is better.\"\n",
    "  * **Informed Decision-Making:** Remember all those choices we had to make?\n",
    "      * What's the best `chunk_size` or `chunk_overlap`?\n",
    "      * Which `embedding_model` captures the semantics of my data best?\n",
    "      * Should I use dense search, hybrid search, or is keyword search enough?\n",
    "      * What's the optimal `k` value (the number of documents to retrieve)?\n",
    "      * Does adding a `reranker` actually help, or just add latency?\n",
    "        Evaluation provides the data needed to answer these questions and justify your choices.\n",
    "  * **Debugging & Improvement:** When your RAG system gives a bad answer, the first suspect is often the retriever. Did it fail to find the relevant document? Or did it pull in a bunch of irrelevant noise that confused the LLM? Evaluation helps you pinpoint these weaknesses in the retrieval pipeline.\n",
    "  * **Building Trustworthy RAG Systems:** The core promise of RAG is to ground LLMs in factual data, reducing hallucinations. If your retriever isn't finding the right facts, you can't trust the final output. Rigorous evaluation is the foundation of a reliable system.\n",
    "  * **Resource Optimization:** Every irrelevant document you feed into the LLM's context window is a waste of computational resources and money. A well-tuned retriever is an efficient retriever, saving you tokens and reducing costs.\n",
    "\n",
    "### The RAG Development Cycle\n",
    "\n",
    "It's crucial to understand that building a RAG application is not a linear process. It's a cycle: **Build -\\> Evaluate -\\> Refine**. You build an initial version, evaluate its performance, analyze the results to find weaknesses, refine your approach, and then evaluate again. This iterative loop is the key to continuous improvement.\n",
    "\n",
    "-----\n",
    "\n",
    "## Core Concepts: Ground Truth and Evaluation Metrics\n",
    "\n",
    "To evaluate anything, you need a standard to measure against. In machine learning, this standard is called **\"ground truth.\"**\n",
    "\n",
    "### Ground Truth: The Source of Truth\n",
    "\n",
    "**Definition:** A ground truth dataset is a collection of test cases, where each case consists of a query and a set of *known relevant documents* from your knowledge base. It's the \"gold standard\" or the \"correct answer key\" for your retriever.\n",
    "\n",
    "**How to Create It:** There's no magic bullet here—creating a ground truth dataset typically requires manual human effort. You (or a domain expert) must sit down, write a representative set of queries you expect your users to ask, and then manually go through your documents to identify which ones are the correct and relevant sources for each query.\n",
    "\n",
    "**Example:**\n",
    "Imagine our knowledge base contains the four documents from our code examples. For a query like `\"How is the Amazon rainforest unique?\"`, our ground truth might look like this:\n",
    "\n",
    "  * **Query:** `\"How is the Amazon rainforest unique?\"`\n",
    "  * **Relevant Documents:**\n",
    "      * `\"doc_id_1\"` (The document about the Amazon being the largest rainforest with incredible biodiversity)\n",
    "      * `\"doc_id_3\"` (The document defining biodiversity, which helps explain a key concept from the first document)\n",
    "\n",
    "Without this \"answer key,\" any metric we calculate is meaningless. You can't know if you're right if you don't know what \"right\" looks like.\n",
    "\n",
    "### Key Retrieval Metrics\n",
    "\n",
    "Once we have our ground truth, we can use several key metrics to score our retriever's performance. Let's focus on the most intuitive and widely used ones.\n",
    "\n",
    "#### Recall\n",
    "\n",
    "  * **Definition:** \"Out of all the truly relevant documents that exist in our knowledge base for a given query, how many did our retriever actually find?\"\n",
    "  * **Formula:** $\\\\text{Recall} = \\\\frac{\\\\text{True Positives}}{\\\\text{True Positives} + \\\\text{False Negatives}} = \\\\frac{\\\\text{Number of relevant docs retrieved}}{\\\\text{Total number of relevant docs in ground truth}}$\n",
    "  * **Analogy:** Imagine you're fishing in a pond that has 10 fish (total relevant documents). You cast your net and catch 7 fish (relevant docs retrieved). Your recall is 7/10 or 0.7. You missed 3 fish (false negatives).\n",
    "  * **Emphasis:** Recall is crucial when it's important to **not miss any relevant information**. You want to minimize \"false negatives.\" For many RAG use cases, high recall is the primary goal for the retrieval step.\n",
    "\n",
    "#### Precision\n",
    "\n",
    "  * **Definition:** \"Out of all the documents our retriever returned, how many were actually relevant?\"\n",
    "  * **Formula:** $\\\\text{Precision} = \\\\frac{\\\\text{True Positives}}{\\\\text{True Positives} + \\\\text{False Positives}} = \\\\frac{\\\\text{Number of relevant docs retrieved}}{\\\\text{Total number of docs retrieved}}$\n",
    "  * **Analogy:** Your librarian hands you a stack of 8 books (total docs retrieved) for your research paper. You find that 6 of them are actually useful (relevant docs retrieved). Your precision is 6/8 or 0.75. The other 2 books were irrelevant noise (false positives).\n",
    "  * **Emphasis:** Precision is about the **quality and signal-to-noise ratio** of your retrieved results. High precision means you're not flooding the LLM's context with junk, which can be distracting and costly.\n",
    "\n",
    "#### Reciprocal Rank (RR) & Mean Reciprocal Rank (MRR)\n",
    "\n",
    "  * **Definition (RR):** This metric cares about the **rank of the *first* correct answer**.\n",
    "      * If the first relevant document is at rank 1, the Reciprocal Rank is $1/1 = 1$.\n",
    "      * If it's at rank 2, the RR is $1/2 = 0.5$.\n",
    "      * If it's at rank 3, the RR is $1/3 \\\\approx 0.33$.\n",
    "      * If no relevant document is found in the results, the RR is 0.\n",
    "  * **Definition (MRR):** The Mean Reciprocal Rank is simply the average of the Reciprocal Ranks across all of your test queries.\n",
    "  * **Emphasis:** MRR is very important for RAG because the LLM might pay more attention to the first few documents it receives. If the most critical piece of information is buried at the bottom of the retrieved list (e.g., at `k=10`), it might be overlooked. A high MRR indicates that your retriever is good at putting the most relevant information right at the top.\n",
    "\n",
    "-----\n",
    "\n",
    "## Practical Methods for Retrieval Evaluation\n",
    "\n",
    "Now let's move from theory to practice. How do we actually go about evaluating our system?\n",
    "\n",
    "### Method 1: Manual Inspection & Human Judgment (The Starting Point)\n",
    "\n",
    "This is the simplest, most fundamental, and often most insightful method. It should always be your starting point.\n",
    "\n",
    "  * **Description:** For a given query, you manually run your retriever and then carefully examine the documents it returns.\n",
    "  * **When to use:** Perfect for early development, debugging specific failing cases, sanity-checking changes, and when working with small datasets.\n",
    "  * **Process:**\n",
    "    1.  Define a small but representative set of test queries.\n",
    "    2.  Run your retriever for each query.\n",
    "    3.  For *each* retrieved document, read its `page_content` and check its `metadata`.\n",
    "    4.  Ask yourself critical questions:\n",
    "          * \"Is this document truly relevant to my query?\"\n",
    "          * \"Does this chunk contain the specific answer, or just related keywords?\"\n",
    "          * \"Would this information help an LLM generate a factually correct answer?\"\n",
    "    5.  Based on your judgment, you can score the documents (e.g., 0 for irrelevant, 1 for relevant) and then manually calculate precision for that query.\n",
    "\n",
    "#### Illustrative Code Example\n",
    "\n",
    "Let's set up a simple retriever and see how we would manually inspect its output.\n",
    "\n",
    "```python\n",
    "# First, ensure you have the necessary libraries installed\n",
    "# pip install langchain langchain-openai langchain-community chromadb beautifulsoup4\n",
    "\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Pro Tip: Set your API key in your environment variables for security.\n",
    "# For this lecture, we'll set it directly if it's not found.\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "# --- 1. Setup (Simulating what you've learned in previous lectures) ---\n",
    "\n",
    "# Here are our source documents for the knowledge base\n",
    "dummy_text_1 = \"The Amazon rainforest is the largest tropical rainforest in the world. It spans across nine countries in South America, including Brazil, Peru, and Colombia. It is renowned for its incredible biodiversity, hosting millions of species of plants, insects, and animals.\"\n",
    "dummy_text_2 = \"Brazil is the largest country in South America and the fifth-largest nation in the world by area. Its official language is Portuguese, and its capital city is Brasília. Brazil is a major exporter of coffee and soybeans.\"\n",
    "dummy_text_3 = \"Biodiversity, short for biological diversity, refers to the variety and variability of life on Earth. It encompasses genetic, species, and ecosystem diversity. Protecting biodiversity is crucial for maintaining stable and healthy ecosystems.\"\n",
    "dummy_text_4 = \"The Sahara Desert is the largest hot desert in the world, stretching across much of North Africa. It is characterized by its vast sand dunes and extremely high temperatures during the day.\"\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=dummy_text_1, metadata={\"source\": \"rainforest_guide.txt\", \"topic\": \"environment\"}),\n",
    "    Document(page_content=dummy_text_2, metadata={\"source\": \"country_facts.pdf\", \"topic\": \"geography\"}),\n",
    "    Document(page_content=dummy_text_3, metadata={\"source\": \"biology_notes.html\", \"topic\": \"science\"}),\n",
    "    Document(page_content=dummy_text_4, metadata={\"source\": \"desert_guide.txt\", \"topic\": \"geography\"}),\n",
    "]\n",
    "\n",
    "# Chunk the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=30)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Instantiate our embedding model and vector store\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "vector_store = Chroma.from_documents(chunks, embeddings_model)\n",
    "\n",
    "# Create our retriever to fetch the top 2 most relevant documents\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# --- 2. Manual Inspection in Action ---\n",
    "\n",
    "# Test Case 1: A straightforward query\n",
    "query_1 = \"What is the largest rainforest known for?\"\n",
    "retrieved_docs_1 = retriever.invoke(query_1)\n",
    "\n",
    "print(f\"Query: '{query_1}'\\n\")\n",
    "print(\"--- Retrieved Documents ---\")\n",
    "for i, doc in enumerate(retrieved_docs_1):\n",
    "    print(f\"Document {i+1} (Source: {doc.metadata.get('source', 'N/A')}):\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")\n",
    "    # HUMAN JUDGMENT POINT:\n",
    "    # Student asks: Is this relevant?\n",
    "    # Doc 1: \"The Amazon rainforest is the largest tropical rainforest...\" -> YES, highly relevant.\n",
    "    # Doc 2: \"renowned for its incredible biodiversity...\" -> YES, also highly relevant.\n",
    "    # Manual Calculation: Precision = 2/2 = 1.0. Looks great!\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test Case 2: A query that might pull in related but less useful info\n",
    "query_2 = \"Tell me about the capital of the biggest country in South America.\"\n",
    "retrieved_docs_2 = retriever.invoke(query_2)\n",
    "\n",
    "print(f\"\\nQuery: '{query_2}'\\n\")\n",
    "print(\"--- Retrieved Documents ---\")\n",
    "for i, doc in enumerate(retrieved_docs_2):\n",
    "    print(f\"Document {i+1} (Source: {doc.metadata.get('source', 'N/A')}):\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")\n",
    "    # HUMAN JUDGMENT POINT:\n",
    "    # Student asks: Is this relevant?\n",
    "    # Doc 1: \"Brazil is the largest country in South America... its capital city is Brasília.\" -> YES, perfectly relevant.\n",
    "    # Doc 2: \"The Amazon rainforest is the largest tropical rainforest... It spans across... Brazil...\" -> This is topically related because of \"Brazil\", but it doesn't answer the question about the capital. It's a \"false positive\" in this context.\n",
    "    # Manual Calculation: Precision = 1/2 = 0.5. Not as good! This might lead us to think about reranking or better chunking.\n",
    "```\n",
    "\n",
    "### Method 2: LLM-as-a-Judge\n",
    "\n",
    "Manual inspection is insightful but doesn't scale. What if you have hundreds of test queries? This is where you can leverage a powerful LLM itself to act as your evaluator.\n",
    "\n",
    "  * **Description:** You use a high-quality LLM (like GPT-4 or Claude 3 Opus) to automate the judgment process. You give it the query, a retrieved document, and a carefully crafted prompt asking it to score the relevance.\n",
    "  * **When to use:** When you need to evaluate at a larger scale than is feasible for humans, and you want to capture nuanced semantic relevance that simple keyword matching would miss.\n",
    "  * **Simplified \"How it works\":** You would create a prompt template that looks something like this:\n",
    "    > \"Given the user query and the retrieved document, please assess the relevance of the document. The document is relevant if it contains information that would help answer the user's query. Please respond with only 'Relevant' or 'Irrelevant'.\n",
    "    > Query: {query}\n",
    "    > Document: {retrieved\\_document\\_content}\"\n",
    "  * **Advantages:** Highly scalable and can be surprisingly accurate at capturing nuanced meaning.\n",
    "  * **Limitations:**\n",
    "      * **Cost:** Using a powerful LLM for evaluation can be expensive.\n",
    "      * **Bias:** The judge LLM can have its own biases or may not understand the context of your specific domain perfectly.\n",
    "      * **Circular Logic:** You're using an LLM to evaluate a system that will feed another LLM. It's not a fully independent check.\n",
    "\n",
    "Due to the complexity of setting up a reliable LLM-as-a-Judge pipeline, we won't do a full code example, but it's a powerful technique to be aware of as you advance.\n",
    "\n",
    "### Method 3: Frameworks for Automated Evaluation\n",
    "\n",
    "For serious, repeatable, and scalable evaluation, you'll want to use a dedicated framework. These tools are built specifically for this purpose and integrate the concepts of ground truth datasets and metrics into a streamlined workflow.\n",
    "\n",
    "#### LangSmith (Strongly Recommended\\!)\n",
    "\n",
    "We've mentioned LangSmith before for observability, but its evaluation capabilities are where it truly shines for RAG developers. It's the best way to connect all the dots.\n",
    "\n",
    "  * **Observability:** The foundation of good evaluation is being able to see what your system is doing. When you enable LangSmith tracing, every call to your retriever is logged. You can click into any run and see the exact query and the `page_content` of every document that was retrieved. This is like manual inspection on steroids.\n",
    "  * **Automated Evals:** LangSmith allows you to formalize your evaluation process. The workflow looks like this:\n",
    "    1.  You create a \"Dataset\" in LangSmith, which is your ground truth (a list of queries and their corresponding expected documents or answers).\n",
    "    2.  You run your RAG chain (your retriever) over this entire dataset.\n",
    "    3.  LangSmith automatically compares the results your retriever produced with the ground truth you provided.\n",
    "    4.  It then calculates and displays metrics like **Precision**, **Recall**, and others for your entire test run.\n",
    "\n",
    "#### Illustrative Code Snippet (Connecting to LangSmith)\n",
    "\n",
    "Getting your chain's activity into LangSmith is a configuration step. Once configured, every run is automatically logged for inspection and evaluation.\n",
    "\n",
    "```python\n",
    "# This is a configuration snippet, not a full evaluation script.\n",
    "# It shows how your chain's activity gets sent to LangSmith.\n",
    "\n",
    "# 1. You need to have the LangSmith library installed:\n",
    "# pip install langsmith\n",
    "\n",
    "# 2. Set the required environment variables in your terminal or .env file:\n",
    "# export LANGCHAIN_TRACING_V2=\"true\"\n",
    "# export LANGCHAIN_API_KEY=\"YOUR_LANGSMITH_API_KEY\"\n",
    "# export LANGCHAIN_PROJECT=\"My RAG Evaluation Project\" # Name your project\n",
    "\n",
    "# Your existing RAG chain setup (no code changes needed here)\n",
    "# For example, using the retriever from our previous example:\n",
    "# rag_chain = (\n",
    "#     {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#     | prompt_template # Assuming you have a prompt\n",
    "#     | llm             # Assuming you have an LLM\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# Now, when you invoke your chain...\n",
    "# response = rag_chain.invoke(\"What is the capital of Brazil?\")\n",
    "\n",
    "# ...the entire trace of that invocation, including the crucial retriever step,\n",
    "# will automatically appear in your LangSmith project online.\n",
    "\n",
    "print(\"With LangSmith environment variables set, running any LangChain object will log traces.\")\n",
    "print(\"Go to the LangSmith platform to see the retrieved documents for each query.\")\n",
    "print(\"This visual feedback is one of the most powerful debugging tools available.\")\n",
    "```\n",
    "\n",
    "**Visualizing a LangSmith Trace:**\n",
    "Imagine you go to the LangSmith UI. You'll see a list of runs. Clicking one shows a waterfall view:\n",
    "\n",
    "1.  **Input:** \"Tell me about the capital of the biggest country in South America.\"\n",
    "2.  **`Retriever` Step:** (You can expand this)\n",
    "      * **Output:** A list of `Document` objects.\n",
    "          * `Document 1`: `page_content=\"Brazil is the largest country...\"`\n",
    "          * `Document 2`: `page_content=\"The Amazon rainforest is the largest...\"`\n",
    "3.  **`LLM` Step:** (You can expand this to see the full prompt sent to the LLM)\n",
    "4.  **Final Output:** \"The capital of Brazil, the largest country in South America, is Brasília.\"\n",
    "\n",
    "This detailed view allows you to immediately spot if the retriever provided the right context.\n",
    "\n",
    "#### Ragas (A Brief Mention)\n",
    "\n",
    "Another excellent tool worth knowing is **Ragas**. It's an open-source framework dedicated specifically to RAG evaluation. It offers a suite of metrics designed to measure different facets of your RAG pipeline, such as:\n",
    "\n",
    "  * **Context Precision & Context Recall:** These are aligned with the precision and recall we discussed, but specifically tailored for the retrieved context.\n",
    "  * **Faithfulness:** Does the final generated answer stick to the facts provided in the retrieved context?\n",
    "  * **Answer Relevancy:** Is the final answer actually relevant to the user's query?\n",
    "\n",
    "Ragas is more advanced and requires a specific setup, but it's a powerful option for those who want to run a comprehensive, open-source evaluation suite locally.\n",
    "\n",
    "-----\n",
    "\n",
    "## Setting Up a Basic Evaluation Dataset (Your Ground Truth)\n",
    "\n",
    "Automated frameworks like LangSmith are powerful, but they need a ground truth dataset to work. Let's look at how you might structure this. The core idea is a collection of `(query, expected_output)` pairs. For retrieval evaluation, the \"output\" is the set of relevant documents.\n",
    "\n",
    "#### Practical Conceptual Exercise\n",
    "\n",
    "Your task is to bridge the gap between your manual inspection and an automated framework. You can start by creating a simple JSON file. For each query, you need to identify the ground truth documents. In the real world, you'd use unique document IDs. For our simple example, let's use keywords we expect to see in the relevant source documents.\n",
    "\n",
    "#### Illustrative Dummy Eval Dataset\n",
    "\n",
    "```python\n",
    "# This is a simplified Python dictionary representing what you might store in a JSON file.\n",
    "# In a real system, `expected_relevant_source` would be a list of document IDs or\n",
    "# exact chunks that your retriever should find.\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"query\": \"What is the largest rainforest in the world?\",\n",
    "        \"expected_relevant_source\": [\"rainforest_guide.txt\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Tell me about the capital of Brazil.\",\n",
    "        \"expected_relevant_source\": [\"country_facts.pdf\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What does biodiversity mean?\",\n",
    "        \"expected_relevant_source\": [\"biology_notes.html\", \"rainforest_guide.txt\"] # Both are relevant!\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are deserts known for?\",\n",
    "        \"expected_relevant_source\": [\"desert_guide.txt\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n--- Example Evaluation Dataset Structure ---\")\n",
    "for i, item in enumerate(eval_dataset):\n",
    "    print(f\"Test Case {i+1}:\")\n",
    "    print(f\"  Query: '{item['query']}'\")\n",
    "    print(f\"  Expected Relevant Source(s): {item['expected_relevant_source']}\\n\")\n",
    "\n",
    "print(\"This structure forms the 'answer key' for automated evaluation tools.\")\n",
    "print(\"The challenge is creating a high-quality, representative set of these test cases.\")\n",
    "\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "## The Iterative Improvement Cycle in Action\n",
    "\n",
    "Evaluation isn't a one-time thing you do at the end. It's the engine of improvement. Here’s how you put it all together in a practical, iterative loop:\n",
    "\n",
    "1.  **Build V1:** Create your initial RAG chain. Choose a reasonable starting point for your `chunk_size`, `embedding_model`, `search_type` (e.g., `similarity`), and `k` value (e.g., `k=4`).\n",
    "2.  **Create Ground Truth:** Build a small but strong evaluation dataset (10-20 queries) with manually labeled ground truth documents.\n",
    "3.  **Run & Evaluate:** Run your retriever on all test queries. Use LangSmith or manual inspection to calculate your baseline metrics (Precision, Recall, MRR).\n",
    "4.  **Analyze the Failures:** This is the most important step. Dig into the results.\n",
    "      * **Low Recall (False Negatives):** Are you missing relevant documents? Why?\n",
    "          * Maybe your `chunk_size` is too large, and specific facts are buried in giant chunks.\n",
    "          * Maybe your embedding model isn't good at understanding the terminology of your domain.\n",
    "          * Maybe `k` is too small, and you're simply not retrieving enough documents.\n",
    "      * **Low Precision (False Positives):** Are you retrieving a lot of irrelevant junk? Why?\n",
    "          * Maybe your chunks are too small and lack context, causing them to match vaguely related queries.\n",
    "          * Maybe similarity search is too broad, and you need the precision of a reranker.\n",
    "5.  **Hypothesize & Adjust:** Based on your analysis, make one change at a time.\n",
    "      * \"I think my chunks are too big. I'll try a smaller `chunk_size` and a larger `chunk_overlap`.\"\n",
    "      * \"My similarity search is bringing back too much noise. I'll add a `CrossEncoderReranker` to my chain.\"\n",
    "      * \"I keep missing documents. I'm going to increase `k` from 4 to 8 and see if my recall improves.\"\n",
    "6.  **Re-evaluate:** Run the exact same evaluation on your modified system. Did the metrics improve? Did recall go up but precision go down?\n",
    "7.  **Repeat:** Continue this cycle of analyzing, adjusting, and re-evaluating until you reach a performance level you're happy with.\n",
    "\n",
    "This methodical, data-driven process is how you build professional, high-quality RAG systems.\n",
    "\n",
    "-----\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "> **Evaluation is Non-Negotiable:** Moving beyond \"it feels right\" to objective, data-driven improvement is the hallmark of a professional RAG developer. It's essential for building trustworthy, efficient, and effective applications.\n",
    ">\n",
    "> **Ground Truth is Your Foundation:** You cannot measure performance without an \"answer key.\" Creating a high-quality ground truth dataset of `(query, relevant_docs)` pairs is the most critical investment you can make in your evaluation process.\n",
    ">\n",
    "> **Know Your Core Metrics:**\n",
    ">\n",
    ">   * **Recall:** Are you finding *all* the relevant stuff? (Minimize missed documents).\n",
    ">   * **Precision:** Is the stuff you're finding *actually* relevant? (Minimize junk).\n",
    ">   * **MRR:** Are you finding the most relevant stuff *first*? (Prioritize rank).\n",
    ">\n",
    "> **Start Simple, Scale with Tools:** Begin with **manual inspection** to build intuition. As you grow, leverage frameworks like **LangSmith** for observability and automated, scalable evaluation against your ground truth dataset.\n",
    ">\n",
    "> **Embrace the Iterative Cycle:** RAG development is a loop: **Build -\\> Evaluate -\\> Analyze -\\> Refine -\\> Repeat**. Every cycle, guided by metrics, makes your system better.\n",
    "\n",
    "-----\n",
    "\n",
    "## Exercises and Thought Experiments\n",
    "\n",
    "1.  **Build Your Own Mini-Eval:**\n",
    "\n",
    "      * Take the RAG system you built in a previous exercise (using a simple text file as your knowledge base).\n",
    "      * Create 5 different queries you think a user might ask.\n",
    "      * For each query, run your retriever (e.g., with `k=3`).\n",
    "      * Manually inspect the 3 retrieved documents. Label each as \"relevant\" (1) or \"irrelevant\" (0).\n",
    "      * For each of your 5 queries, calculate the **Precision**. What's the average precision across all queries?\n",
    "\n",
    "2.  **The \"False Negative\" Hunt:**\n",
    "\n",
    "      * Write a very specific query where you *know* the answer exists in a single sentence within your source document.\n",
    "      * Run your retriever. What happens if it *doesn't* return the chunk containing that sentence? This is a \"false negative.\"\n",
    "      * What are your first three hypotheses for why this failed? (e.g., chunk splitting separated the key sentence from its context, the embedding model didn't capture the query's intent, `k` was too small, etc.). How would you test these hypotheses?\n",
    "\n",
    "3.  **The \"False Positive\" Dilemma:**\n",
    "\n",
    "      * Write a query that is thematically broad (e.g., if your document is about cloud computing, ask \"What are some technology challenges?\").\n",
    "      * Look at the retrieved documents. You will likely get some that are semantically similar but not directly useful for answering the question (e.g., a chunk about billing, which is a \"challenge\" but maybe not the technical one you wanted). This is a \"false positive.\"\n",
    "      * How could a reranker potentially solve this? Why would a cross-encoder, which looks at the query and document *together*, be better at filtering these out than the vector search alone?\n",
    "\n",
    "4.  **LangSmith Exploration (Recommended):**\n",
    "\n",
    "      * If you haven't already, sign up for a free LangSmith account.\n",
    "      * Configure the environment variables in your project as shown in the lecture.\n",
    "      * Run your RAG chain for 5-10 different queries.\n",
    "      * Go to the LangSmith web UI and find your project.\n",
    "      * Click into the trace for a query that gave a poor result. Expand the \"Retriever\" step. Analyze the documents it chose. Did it make a mistake you can now clearly see? This hands-on experience is invaluable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34681a8c-35ea-481b-a419-f83b3dc45f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
